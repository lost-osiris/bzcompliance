{
  "bugs": [
    {
      "cf_qe_conditional_nak": [],
      "cf_story_points": "---",
      "cf_pm_score": "2000",
      "estimated_time": "0.0",
      "depends_on": [],
      "cf_regression_status": "---",
      "cf_conditional_nak": [],
      "creation_time": "20120607T13:17:00",
      "product": "Red Hat Enterprise Linux 6",
      "actual_time": "0.0",
      "docs_contact": "",
      "is_open": "False",
      "keywords": [
        "ZStream"
      ],
      "target_release": [
        "6.4"
      ],
      "external_bugs": [],
      "id": "829739",
      "cf_release_notes": "Previously, on Fibre Channel hosts using the QLogic QLA2xxx driver, users could encounter error messages and long I/O outages during fabric faults. This was because the number of outstanding requests was hard-coded. With this update, the number of outstanding requests the driver keeps track of is based on the available resources instead of being hard-coded, which avoids the aforementioned problems.",
      "priority": "urgent",
      "severity": "urgent",
      "is_confirmed": "True",
      "is_creator_accessible": "True",
      "cf_fixed_in": "kernel-2.6.32-350.el6",
      "creator": "ayyanar@netapp.com",
      "comments": [
        {
          "count": "0",
          "attachment_id": "590198",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 590198\nmessages file with qla2xxx verbosity as 0x1e400000\n\nDescription of problem:\n\nDuring IO with fabric faults, one generally sees several ''kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff88028ea52980'' messages in the syslog as shown below when QLogic driver verbosity is set to 0x1e400000:\n\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff880076ba1580.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff8800769cb4c0.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff8800769cb5c0.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff88007713b280.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff880076ba1580.\nJun  7 17:29:01 ibmx3650-210-104 kernel: Start scsi failed rval=258 for cmd=ffff8800769cb2c0.\n\nVersion-Release number of selected component (if applicable):\n\nRHEL6U3 alpha kernel onwards.\n\nmodel description: QLogic QLE2562\ndriver version:    v.8.04.00.04.06.3-k-debug\nfirmware version:  v.5.06.05 (90d5)\n\nHow reproducible:\n\nAlways.\n\nSteps to Reproduce:\n\n1.Map 20*10G (4path each)LUN's from NetApp array to QLogic FC host.\n2.Set QLogic driver verbosity to 0x1e400000\n3.Create PV on multipath device and 3 LV's of 100G each.\n4.Create ext4 FS on LV's and mount them.\n5.Start IO on mount points and then introduce storage fault.",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120607T13:17:03",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120607T13:17:03",
          "id": "4764852",
          "is_private": "False"
        },
        {
          "count": "1",
          "creator": "pm-rhel@redhat.com",
          "text": "Since this issue was entered in bugzilla, the release flag has been\nset to ? to ensure that it is properly evaluated for this release.",
          "author": "pm-rhel@redhat.com",
          "creation_time": "20120607T13:22:24",
          "bug_id": "829739",
          "creator_id": "193983",
          "time": "20120607T13:22:24",
          "id": "4764860",
          "is_private": "True"
        },
        {
          "count": "2",
          "creator": "marting@netapp.com",
          "text": "The above 'Start scsi failed' error messages are seen consistently during perturbations in our tests. As per Chad, these indicate an error when queuing a request on the request queue and should not be seen.\n\nSecondly, with the standard QLogic logging (0x1e400000) enabled, several 'fc_remote_port_chkready failed' messages are also seen as shown below:\n\nkernel: qla2xxx [0000:1a:00.0]-3803:6: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\nkernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\nkernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\n\nThese as per Chad are benign, and is only an indication that the queuecommand handler is called when the rport gets blocked. But since these flood the /var/log/messages which may cause the host to be sluggish, I would request QLogic to log these messages only with higher logging enabled i.e. like say 0x7fffffff.",
          "author": "marting@netapp.com",
          "creation_time": "20120607T14:06:37",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120607T14:06:37",
          "id": "4765016",
          "is_private": "False"
        },
        {
          "count": "3",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #2)\n> The above 'Start scsi failed' error messages are seen consistently during\n> perturbations in our tests. As per Chad, these indicate an error when\n> queuing a request on the request queue and should not be seen.\n\nI'll create a debug patch to print where in qla24xx_start_scsi we're failing so that we can trace where the actual failure condition is.\n\n> \n> Secondly, with the standard QLogic logging (0x1e400000) enabled, several\n> 'fc_remote_port_chkready failed' messages are also seen as shown below:\n> \n> kernel: qla2xxx [0000:1a:00.0]-3803:6: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> kernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> kernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> \n> These as per Chad are benign, and is only an indication that the\n> queuecommand handler is called when the rport gets blocked. But since these\n> flood the /var/log/messages which may cause the host to be sluggish, I would\n> request QLogic to log these messages only with higher logging enabled i.e.\n> like say 0x7fffffff.\n\nWould it be possible to open a separate bugzilla for this message if only so that we don't confuse the two issues?",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120611T14:25:33",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120611T14:25:33",
          "id": "4772279",
          "is_private": "False"
        },
        {
          "count": "4",
          "creator": "ayyanar@netapp.com",
          "text": "> I'll create a debug patch to print where in qla24xx_start_scsi we're failing\n> so that we can trace where the actual failure condition is.\n\nCan you give this patch?\n\n> > Secondly, with the standard QLogic logging (0x1e400000) enabled, several\n> > 'fc_remote_port_chkready failed' messages are also seen as shown below:\n\n> Would it be possible to open a separate bugzilla for this message if only so\n> that we don't confuse the two issues?\n\nSure.Will do.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120613T08:37:07",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120613T08:37:07",
          "id": "4821001",
          "is_private": "False"
        },
        {
          "count": "5",
          "creator": "ayyanar@netapp.com",
          "text": "\n> > Would it be possible to open a separate bugzilla for this message if only so\n> > that we don't confuse the two issues?\n\nFiled a separate bug 831529 for this. Is this patch ready?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120615T08:21:01",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120615T08:21:01",
          "id": "4827744",
          "is_private": "False"
        },
        {
          "count": "6",
          "attachment_id": "592195",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 592195\nqla2xxx: Instrument queuing errors in qla24xx_start_scsi.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20120615T17:55:09",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120615T17:55:09",
          "id": "4829305",
          "is_private": "False"
        },
        {
          "count": "7",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #5)\n> > > Would it be possible to open a separate bugzilla for this message if only so\n> > > that we don't confuse the two issues?\n> \n> Filed a separate bug 831529 for this. Is this patch ready?\n\nYes, patch is the attachment from Comment 6.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120615T17:55:47",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120615T17:55:47",
          "id": "4829306",
          "is_private": "False"
        },
        {
          "count": "8",
          "creator": "ayyanar@netapp.com",
          "text": "Ran test with patched kernel in comment 6 and message file is attached.\n\nSoon after IO started, I am seeing below messages:\n\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No room left in outstanding commands array, index=1024.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4: Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3031:5: No room left in outstanding commands array, index=1024.\n\nFew kernel hung_task_timeout_secs also seen with dt.stable. Still continuing my test.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120618T12:59:56",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120618T12:59:56",
          "id": "4833132",
          "is_private": "False"
        },
        {
          "count": "9",
          "attachment_id": "592636",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 592636\nmessages file with patch in comment 6",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120618T13:01:21",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120618T13:01:21",
          "id": "4833144",
          "is_private": "False"
        },
        {
          "count": "10",
          "attachment_id": "592970",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 592970\nmessages file_2 with patch in comment 6",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120619T14:03:46",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120619T14:03:46",
          "id": "4836600",
          "is_private": "False"
        },
        {
          "count": "11",
          "creator": "cdupuis@redhat.com",
          "text": "Just curious looks like we start getting the messages:\n\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No room left in outstanding commands array, index=1024.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4: Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\n\neven before we start any type of perturbations?",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120619T19:39:00",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120619T19:39:00",
          "id": "4840151",
          "is_private": "False"
        },
        {
          "count": "12",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to comment #11)\n> Just curious looks like we start getting the messages:\n> \n> Jun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No\n> room left in outstanding commands array, index=1024.\n> Jun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4:\n> Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\n> \n> even before we start any type of perturbations?\n\nYes. Once IO started, we are seeing above messages.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120620T09:22:35",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120620T09:22:35",
          "id": "4842765",
          "is_private": "False"
        },
        {
          "count": "13",
          "creator": "marting@netapp.com",
          "text": "Chad,\n\nAny updates here? \n\nGiven that we usually run heavy IO in our tests here, does this mean that the QLogic default of 1024 max outstanding commands in the ISP queue is insufficient for such scenarios, and this probably needs to be bumped up?",
          "author": "marting@netapp.com",
          "creation_time": "20120625T11:58:52",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120625T11:58:52",
          "id": "4856909",
          "is_private": "False"
        },
        {
          "count": "14",
          "creator": "pm-rhel@redhat.com",
          "text": "This request was evaluated by Red Hat Product Management for\ninclusion in a Red Hat Enterprise Linux release.  Product\nManagement has requested further review of this request by\nRed Hat Engineering, for potential inclusion in a Red Hat\nEnterprise Linux release for currently deployed products.\nThis request is not yet committed for inclusion in a release.",
          "author": "pm-rhel@redhat.com",
          "creation_time": "20120627T13:47:36",
          "bug_id": "829739",
          "creator_id": "193983",
          "time": "20120627T13:47:36",
          "id": "4862962",
          "is_private": "False"
        },
        {
          "count": "15",
          "attachment_id": "595143",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 595143\nqla2xxx: Panic when outstanding_cmds array is full to get a snapshot of the system for debugging.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20120628T21:25:12",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120628T21:25:12",
          "id": "4866361",
          "is_private": "False"
        },
        {
          "count": "16",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #13)\n> Chad,\n> \n> Any updates here? \n> \n> Given that we usually run heavy IO in our tests here, does this mean that\n> the QLogic default of 1024 max outstanding commands in the ISP queue is\n> insufficient for such scenarios, and this probably needs to be bumped up?\n\nThat's a possibility Martin, but first we need to see what the contents of the oustanding_cmds array are to see what type of commands are filling it up.  The easiest way to do that at this point is to basically panic the system when this condition occurs to generate a vmcore that we can analyze.  If this is possible, I've attached a patch in comment 15 that will panic the system in the right spot.\n\nLet me know if this is doable.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120628T21:27:52",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120628T21:27:52",
          "id": "4866366",
          "is_private": "False"
        },
        {
          "count": "17",
          "creator": "ayyanar@netapp.com",
          "text": "Ran with the patch given on comment 5 and hit panic. \n\nKernel panic - not syncing: qla2xxx: oustanding_cmds array on scsi_host 2 is full\n\nPid: 37, comm: kblockd/3 Tainted: G           ---------------  T 2.6.32-279.el6.panic.bz829739.x86_64 #1\nCall Trace:\n [<ffffffff814fd11a>] ? panic+0xa0/0x168\n [<ffffffff811625e0>] ? cache_alloc_refill+0x1c0/0x240\n [<ffffffffa01f4f16>] ? qla24xx_start_scsi+0x4e6/0x650 [qla2xxx]\n [<ffffffff81116855>] ? mempool_alloc_slab+0x15/0x20\n [<ffffffffa01f5876>] ? qla24xx_dif_start_scsi+0x7f6/0x13f0 [qla2xxx]\n [<ffffffff812815a0>] ? sg_init_table+0x30/0x50\n [<ffffffff8128163e>] ? __sg_alloc_table+0x7e/0x130\n [<ffffffff8136b5e0>] ? scsi_sg_alloc+0x0/0x60\n [<ffffffff81116855>] ? mempool_alloc_slab+0x15/0x20\n [<ffffffff81116963>] ? mempool_alloc+0x63/0x140\n [<ffffffff8136b9f9>] ? scsi_setup_fs_cmnd+0x79/0xe0\n [<ffffffff81363180>] ? scsi_done+0x0/0x60\n [<ffffffffa01dbdbc>] ? qla2xxx_queuecommand+0x2fc/0x370 [qla2xxx]\n [<ffffffff81363411>] ? scsi_dispatch_cmd+0x101/0x360\n [<ffffffff8136af0d>] ? scsi_request_fn+0x41d/0x790\n [<ffffffff81253de3>] ? ftrace_raw_event_id_block_rq+0x153/0x190\n [<ffffffff812554c1>] ? __blk_run_queue+0x31/0x40\n [<ffffffff8124f6f8>] ? elv_insert+0xf8/0x1a0\n [<ffffffff8124f7ea>] ? __elv_add_request+0x4a/0x90\n [<ffffffff81254ead>] ? blk_insert_cloned_request+0x7d/0xc0\n [<ffffffffa00023ac>] ? dm_dispatch_request+0x3c/0x70 [dm_mod]\n [<ffffffffa000387a>] ? dm_request_fn+0x18a/0x290 [dm_mod]\n [<ffffffff81255662>] ? __generic_unplug_device+0x32/0x40\n [<ffffffff8125569e>] ? generic_unplug_device+0x2e/0x50\n [<ffffffffa0002788>] ? dm_unplug_all+0x68/0x70 [dm_mod]\n [<ffffffff812503b6>] ? blk_unplug_work+0x36/0x70\n [<ffffffff81250380>] ? blk_unplug_work+0x0/0x70\n [<ffffffff8108c760>] ? worker_thread+0x170/0x2a0\n [<ffffffff810920d0>] ? autoremove_wake_function+0x0/0x40\n [<ffffffff8108c5f0>] ? worker_thread+0x0/0x2a0\n [<ffffffff81091d66>] ? kthread+0x96/0xa0\n [<ffffffff8100c14a>] ? child_rip+0xa/0x20\n [<ffffffff81091cd0>] ? kthread+0x0/0xa0\n [<ffffffff8100c140>] ? child_rip+0x0/0x20\ndo_IRQ: 0.137 No irq handler for vector (irq -1)\n\u00ffirq 16: nobody cared (try booting with the \"irqpoll\" option)\n \nBut unable to collect vmcore file due to below error on console:\n\nDo you have a strange power saving mode enabled?\nDazed and confused, but trying to continue\nUhhuh. NMI received for unknown reason 35 on CPU 0. Will retry and update.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120703T15:21:35",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120703T15:21:35",
          "id": "4873816",
          "is_private": "False"
        },
        {
          "count": "18",
          "creator": "cdupuis@redhat.com",
          "text": ">  \n> But unable to collect vmcore file due to below error on console:\n> \n> Do you have a strange power saving mode enabled?\n> Dazed and confused, but trying to continue\n> Uhhuh. NMI received for unknown reason 35 on CPU 0. Will retry and update.\n\nCrud.  Does this happen even if you trigger a test crash via /proc/sysrq-trigger?  If we can't get a crash dump I can create a patch to capture the information using counters and print it to the error log.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120703T17:35:29",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120703T17:35:29",
          "id": "4874251",
          "is_private": "False"
        },
        {
          "count": "19",
          "creator": "ayyanar@netapp.com",
          "text": "Hit panic again on FCoE QLogic host and copied vmcore.flat file here:\n\nftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2  \n\nback trace is as follows:\n\ncrash> bt -a\nPID: 24807  TASK: ffff88021764aae0  CPU: 0   COMMAND: ''dt.stable''\n #0 [ffff880040207e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040207ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040207ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040207ef0] notify_die at ffffffff810980ae\n #4 [ffff880040207f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040207f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+50]\n    RIP: ffffffff81500012  RSP: ffff88021fb013d8  RFLAGS: 00000093\n    RAX: 0000000000008d5e  RBX: ffff880226974680  RCX: 0000000000008d5c\n    RDX: 0000000000000246  RSI: 0000000000011220  RDI: ffff880224f73840\n    RBP: ffff88021fb013d8   R8: ffff8801c1cb3e38   R9: 0000000000000000\n    R10: 0000000000000000  R11: 000000000000040c  R12: ffff880224f735e0\n    R13: ffff8802245a4000  R14: ffff880226d72080  R15: ffff880224f73800\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff88021fb013d8] _spin_lock_irqsave at ffffffff81500012\n #7 [ffff88021fb013e0] qla24xx_start_scsi at ffffffffa01f4af7 [qla2xxx]\n #8 [ffff88021fb01490] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #9 [ffff88021fb01620] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n#10 [ffff88021fb01680] scsi_dispatch_cmd at ffffffff81363411\n#11 [ffff88021fb016b0] scsi_request_fn at ffffffff8136af0d\n#12 [ffff88021fb01730] __blk_run_queue at ffffffff812554c1\n#13 [ffff88021fb01750] elv_insert at ffffffff8124f6f8\n#14 [ffff88021fb01790] __elv_add_request at ffffffff8124f7ea\n#15 [ffff88021fb017c0] blk_insert_cloned_request at ffffffff81254ead\n#16 [ffff88021fb017f0] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#17 [ffff88021fb01810] dm_request_fn at ffffffffa000387a [dm_mod]\n#18 [ffff88021fb01870] __generic_unplug_device at ffffffff81255662\n#19 [ffff88021fb01890] generic_unplug_device at ffffffff8125569e\n#20 [ffff88021fb018b0] dm_unplug_all at ffffffffa0002788 [dm_mod]\n#21 [ffff88021fb018e0] blk_unplug at ffffffff81250324\n#22 [ffff88021fb01900] dm_table_unplug_all at ffffffffa00041fc [dm_mod]\n#23 [ffff88021fb01970] dm_unplug_all at ffffffffa0002768 [dm_mod]\n#24 [ffff88021fb019a0] blk_unplug at ffffffff81250324\n#25 [ffff88021fb019c0] blk_backing_dev_unplug at ffffffff81250372\n#26 [ffff88021fb019d0] block_sync_page at ffffffff811ac57e\n#27 [ffff88021fb019e0] sync_page at ffffffff811140f8\n#28 [ffff88021fb019f0] __wait_on_bit at ffffffff814fe97f\n#29 [ffff88021fb01a40] wait_on_page_bit at ffffffff81114333\n#30 [ffff88021fb01aa0] grab_cache_page_write_begin at ffffffff81115250\n#31 [ffff88021fb01af0] ext4_da_write_begin at ffffffffa026d9f4 [ext4]\n#32 [ffff88021fb01b90] generic_file_buffered_write at ffffffff81114ab3\n#33 [ffff88021fb01c60] __generic_file_aio_write at ffffffff81116450\n#34 [ffff88021fb01d20] generic_file_aio_write at ffffffff811166ef\n#35 [ffff88021fb01d70] ext4_file_write at ffffffffa0262131 [ext4]\n#36 [ffff88021fb01dc0] do_sync_write at ffffffff8117ad6a\n#37 [ffff88021fb01ef0] vfs_write at ffffffff8117b068\n#38 [ffff88021fb01f30] sys_write at ffffffff8117ba81\n#39 [ffff88021fb01f80] sysenter_dispatch at ffffffff8104a820\n    RIP: 00000000f779e430  RSP: 00000000ff997d64  RFLAGS: 00000296\n    RAX: ffffffffffffffda  RBX: ffffffff8104a820  RCX: 00000000085ed000\n    RDX: 000000000000c800  RSI: 00000000085d7060  RDI: 0000000000000000\n    RBP: 00000000ff997da8   R8: 0000000000000000   R9: 0000000000000000\n    R10: 0000000000000000  R11: 0000000000000000  R12: 0000000000000000\n    R13: 0000000000000000  R14: 0000000000000003  R15: 0000000000000000\n    ORIG_RAX: 0000000000000004  CS: 0023  SS: 002b\n\nPID: 24810  TASK: ffff880220e76aa0  CPU: 1   COMMAND: ''dt.stable''\n #0 [ffff8800402837d8] machine_kexec at ffffffff8103281b\n #1 [ffff880040283838] crash_kexec at ffffffff810ba662\n #2 [ffff880040283908] panic at ffffffff814fd121\n #3 [ffff880040283988] qla24xx_start_scsi at ffffffffa01f4f16 [qla2xxx]\n #4 [ffff880040283a38] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #5 [ffff880040283bc8] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n #6 [ffff880040283c28] scsi_dispatch_cmd at ffffffff81363411\n #7 [ffff880040283c58] scsi_request_fn at ffffffff8136af0d\n #8 [ffff880040283cd8] __blk_run_queue at ffffffff812554c1\n #9 [ffff880040283cf8] elv_insert at ffffffff8124f6f8\n#10 [ffff880040283d38] __elv_add_request at ffffffff8124f7ea\n#11 [ffff880040283d68] blk_insert_cloned_request at ffffffff81254ead\n#12 [ffff880040283d98] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#13 [ffff880040283db8] dm_request_fn at ffffffffa000387a [dm_mod]\n#14 [ffff880040283e18] __blk_run_queue at ffffffff812554c1\n#15 [ffff880040283e38] blk_run_queue at ffffffff81255610\n#16 [ffff880040283e58] rq_completed at ffffffffa0001dac [dm_mod]\n#17 [ffff880040283e78] dm_softirq_done at ffffffffa000257f [dm_mod]\n#18 [ffff880040283eb8] blk_done_softirq at ffffffff8125d605\n#19 [ffff880040283ee8] __do_softirq at ffffffff81073ec1\n#20 [ffff880040283f58] call_softirq at ffffffff8100c24c\n#21 [ffff880040283f70] do_softirq at ffffffff8100de85\n#22 [ffff880040283f90] irq_exit at ffffffff81073ca5\n#23 [ffff880040283fa0] smp_call_function_single_interrupt at ffffffff8102a905\n#24 [ffff880040283fb0] call_function_single_interrupt at ffffffff8100bdb3\n--- <IRQ stack> ---\n#25 [ffff88021fb07f58] call_function_single_interrupt at ffffffff8100bdb3\n    RIP: 00000000080585a2  RSP: 00000000ff8baf40  RFLAGS: 00000246\n    RAX: 0000000000000042  RBX: 00000000ff8baf58  RCX: 0000000000000000\n    RDX: 000000000000009d  RSI: 000000000848749d  RDI: 000000000000849d\n    RBP: ffffffff8100bdae   R8: 0000000000000000   R9: 0000000000000000\n    R10: 0000000000000000  R11: 0000000000000000  R12: 0000000000000000\n    R13: 0000000000000000  R14: 0000000000000000  R15: 0000000000000000\n    ORIG_RAX: ffffffffffffff04  CS: 0023  SS: 002b\n\nPID: 36     TASK: ffff88022a4d0040  CPU: 2   COMMAND: ''kblockd/2''\n #0 [ffff880040307e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040307ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040307ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040307ef0] notify_die at ffffffff810980ae\n #4 [ffff880040307f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040307f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+47]\n    RIP: ffffffff8150000f  RSP: ffff88022a4d9900  RFLAGS: 00000093\n    RAX: 0000000000008d5f  RBX: ffff8801c1efb080  RCX: 0000000000008d5c\n    RDX: 0000000000000246  RSI: 0000000000011220  RDI: ffff880224f73840\n    RBP: ffff88022a4d9900   R8: ffff8801a5f49508   R9: 0000000000000000\n    R10: 0000000000000000  R11: 000000000000040d  R12: ffff880224f735e0\n    R13: ffff8802245a4000  R14: ffff880226d05380  R15: ffff880224f73800\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff88022a4d9900] _spin_lock_irqsave at ffffffff8150000f\n #7 [ffff88022a4d9908] qla24xx_start_scsi at ffffffffa01f4af7 [qla2xxx]\n #8 [ffff88022a4d99b8] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #9 [ffff88022a4d9b48] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n#10 [ffff88022a4d9ba8] scsi_dispatch_cmd at ffffffff81363411\n#11 [ffff88022a4d9bd8] scsi_request_fn at ffffffff8136af0d\n#12 [ffff88022a4d9c58] __blk_run_queue at ffffffff812554c1\n#13 [ffff88022a4d9c78] elv_insert at ffffffff8124f6f8\n#14 [ffff88022a4d9cb8] __elv_add_request at ffffffff8124f7ea\n#15 [ffff88022a4d9ce8] blk_insert_cloned_request at ffffffff81254ead\n#16 [ffff88022a4d9d18] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#17 [ffff88022a4d9d38] dm_request_fn at ffffffffa000387a [dm_mod]\n#18 [ffff88022a4d9d98] __generic_unplug_device at ffffffff81255662\n#19 [ffff88022a4d9db8] generic_unplug_device at ffffffff8125569e\n#20 [ffff88022a4d9dd8] dm_unplug_all at ffffffffa0002788 [dm_mod]\n#21 [ffff88022a4d9e08] blk_unplug_work at ffffffff812503b6\n#22 [ffff88022a4d9e38] worker_thread at ffffffff8108c760\n#23 [ffff88022a4d9ee8] kthread at ffffffff81091d66\n#24 [ffff88022a4d9f48] kernel_thread at ffffffff8100c14a\n\nPID: 0      TASK: ffff88022ae2eaa0  CPU: 3   COMMAND: ''swapper''\n #0 [ffff880040387e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040387ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040387ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040387ef0] notify_die at ffffffff810980ae\n #4 [ffff880040387f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040387f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+47]\n    RIP: ffffffff8150000f  RSP: ffff880040383e88  RFLAGS: 00000097\n    RAX: 0000000000008d5d  RBX: ffff880224f73800  RCX: 0000000000008d5c\n    RDX: 0000000000000086  RSI: ffff880225b3fec0  RDI: ffff880224f73840\n    RBP: ffff880040383e88   R8: 0000000000010000   R9: ffff88022ae37ee8\n    R10: 0000000000000000  R11: 0000000000000001  R12: ffff880224f73840\n    R13: 0000000000000000  R14: ffffc90000c3c000  R15: ffff880225b3fec0\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff880040383e88] _spin_lock_irqsave at ffffffff8150000f\n #7 [ffff880040383e90] qla24xx_msix_rsp_q at ffffffffa01f863d [qla2xxx]\n #8 [ffff880040383ed0] handle_IRQ_event at ffffffff810db800\n #9 [ffff880040383f20] handle_edge_irq at ffffffff810ddf8e\n#10 [ffff880040383f60] handle_irq at ffffffff8100df09\n#11 [ffff880040383f80] do_IRQ at ffffffff81505aec\n--- <IRQ stack> ---\n#12 [ffff88022ae37e38] ret_from_intr at ffffffff8100ba53\n    [exception RIP: mwait_idle+119]\n    RIP: ffffffff81014877  RSP: ffff88022ae37ee8  RFLAGS: 00000246\n    RAX: 0000000000000000  RBX: ffff88022ae37ef8  RCX: 0000000000000000\n    RDX: 0000000000000000  RSI: ffff88022ae37fd8  RDI: ffff88022ac5d840\n    RBP: ffffffff8100ba4e   R8: 0000000000000000   R9: 0000000000000001\n    R10: 0000000000000000  R11: 0000000000000000  R12: ffffffff81bde6d0\n    R13: 0000000000000000  R14: ffffffff810f31d3  R15: ffff88022ae37e68\n    ORIG_RAX: ffffffffffffff76  CS: 0010  SS: 0018\n#13 [ffff88022ae37f00] cpu_idle at ffffffff81009e06\ncrash>",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120703T18:08:59",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120703T18:08:59",
          "id": "4874304",
          "is_private": "False"
        },
        {
          "count": "20",
          "creator": "cdupuis@redhat.com",
          "text": "There doesn't appear to be a file at ftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120703T19:02:53",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120703T19:02:53",
          "id": "4874380",
          "is_private": "False"
        },
        {
          "count": "21",
          "creator": "ayyanar@netapp.com",
          "text": "\n(In reply to comment #20)\n> There doesn't appear to be a file at\n> ftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2.\n\nIt should work now. please check.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120703T19:38:16",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120703T19:38:16",
          "id": "4874489",
          "is_private": "False"
        },
        {
          "count": "22",
          "creator": "cdupuis@redhat.com",
          "text": "Works now, thanks.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120703T19:56:46",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120703T19:56:46",
          "id": "4874537",
          "is_private": "False"
        },
        {
          "count": "23",
          "creator": "ayyanar@netapp.com",
          "text": "Chad, any update on this? Do you need any other logs to triage?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120709T11:18:35",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120709T11:18:35",
          "id": "4882739",
          "is_private": "False"
        },
        {
          "count": "24",
          "attachment_id": "597898",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 597898\nqla2xxx: BZ829739 composite patch 7/12/2012.\n\n- Still panics if the oustanding commands array fills up\n- Increases the outstanding commands array to 2048.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20120712T20:30:22",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120712T20:30:22",
          "id": "4896468",
          "is_private": "False"
        },
        {
          "count": "25",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #23)\n> Chad, any update on this? Do you need any other logs to triage?\n\nCan you try the patch in comment 24?  I looked at the vmcore and:\n\na) The oustanding commands array is for a particular request queue really is filling up.\nb) The are all SCSI commands.\n\nI had wanted the dump to make sure that there weren't a lot of command internal to the driver that were outstanding.  Anyways, as a data point, let's try increasing the capacity of how many commands we can have outstanding as that is the best course of action at this point.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120712T20:33:36",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120712T20:33:36",
          "id": "4896475",
          "is_private": "False"
        },
        {
          "count": "26",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #25)\n> \n> Can you try the patch in comment 24?  I looked at the vmcore and:\n\nActually can you hold off on that...if we hit the condition again, we need to collect a firmware dump so I'll need to change the code slightly.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120712T22:14:09",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120712T22:14:09",
          "id": "4896616",
          "is_private": "False"
        },
        {
          "count": "27",
          "creator": "ayyanar@netapp.com",
          "text": "Chad, Just a data point to add here ,we have already tried increasing the MAX_OUTSTANDING_COMMANDS to 16384 and we are still seeing ''Start scsi failed'' messages after 18 iterations of faults. (after 16 hrs of IO with faults)\n\nJul 12 15:42:02 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077215a80.\n\nI hope oustanding commands array is quickly filling up. Do you want us to try with MAX_OUTSTANDING_COMMANDS=2048 and collect firmware dumps and vmcore Or MAX_OUTSTANDING_COMMANDS=16384 ??\n\nTo collect firmware dump can I use below patch along with firmware dump collect scripts?\n\n--- qla_iocb.c.orig     2012-07-13 18:24:32.970553006 +0530\n+++ qla_iocb.c  2012-07-13 18:30:25.057512551 +0530\n@@ -363,7 +363,11 @@ qla2x00_start_scsi(srb_t *sp)\n                        break;\n        }\n        if (index == MAX_OUTSTANDING_COMMANDS)\n-               goto queuing_error;\n+               /* Try to take a firmware dump to ascertain the\n+                        * state of the firmware */\n+                       ha->isp_ops->fw_dump(vha, 0);\n+             panic(''scsi(%ld): req->outstanding_cmds is full\\n'', vha->host_no);\n+              goto queuing_error;\n\n        /* Map the sg table so we have an accurate count of sg entries needed */\n        if (scsi_sg_count(cmd)) {",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120713T13:14:25",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120713T13:14:25",
          "id": "4897895",
          "is_private": "False"
        },
        {
          "count": "28",
          "creator": "ayyanar@netapp.com",
          "text": "One more info to share: with MAX_OUTSTANDING_COMMANDS=16384 first 18hrs including host FC port block (disable and enable) testing there is no ''Start sci failed: messages; after 18 hrs of faults with IO, I am seeing ''Start scsi failed'' messages only with host FC port block (disable and enable) testing. \n\nIt happens with one particular port (in this host it is fc host2)goes offline. \n\nFrom the logs,\n\nJul 12 15:40:33 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 12 15:42:02 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077215a80.\nJul 12 15:43:05 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 12 23:44:49 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)]\nJul 12 23:46:19 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff8802831490c0.\nJul 12 23:47:21 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 06:29:57 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 06:31:50 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880283214580\nJul 13 06:32:30 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 10:41:07 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 10:43:14 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff88006fecdcc0.\nJul 13 10:43:39 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 16:28:43 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 16:29:10 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077d27c80\nJul 13 16:31:15 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120713T13:52:46",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120713T13:52:46",
          "id": "4898006",
          "is_private": "False"
        },
        {
          "count": "29",
          "creator": "ayyanar@netapp.com",
          "text": "> Actually can you hold off on that...if we hit the condition again, we need\n> to collect a firmware dump so I'll need to change the code slightly.\n\nChad, can you give this patch?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120717T18:43:06",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120717T18:43:06",
          "id": "4905039",
          "is_private": "False"
        },
        {
          "count": "30",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #29)\n> > Actually can you hold off on that...if we hit the condition again, we need\n> > to collect a firmware dump so I'll need to change the code slightly.\n> \n> Chad, can you give this patch?\n\nThe firmware dump is probably not needed at this point.  There is just a large number of commands that are submitted in a short time that we're not getting responses for to clear out the outstanding_cmds array.  It's possible that the fact that this occurs when a port goes down has something to do with the behavior.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120717T21:30:50",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120717T21:30:50",
          "id": "4905326",
          "is_private": "False"
        },
        {
          "count": "31",
          "creator": "ayyanar@netapp.com",
          "text": "> The firmware dump is probably not needed at this point.  There is just a\n> large number of commands that are submitted in a short time that we're not\n> getting responses for to clear out the outstanding_cmds array.  It's\n> possible that the fact that this occurs when a port goes down has something\n> to do with the behavior.\nCan you create a patch to clean out the outstanding_cmds array? Or you want me to try with MAX_OUTSTANDING_COMMANDS=2048 and collect vmcore?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120718T13:28:18",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120718T13:28:18",
          "id": "4906711",
          "is_private": "False"
        },
        {
          "count": "32",
          "creator": "marting@netapp.com",
          "text": "(In reply to comment #30)\n> The firmware dump is probably not needed at this point.  There is just a\n> large number of commands that are submitted in a short time that we're not\n> getting responses for to clear out the outstanding_cmds array.  It's\n> possible that the fact that this occurs when a port goes down has something\n> to do with the behavior.\n\nPoint is with the current default of 1024 max outstanding commands in the ISP queue, we are hitting the problem consistently during straight IO itself i.e. even before fabric faults are introduced. But after bumping this up to a higher value like say 16384, the issue is harder to reproduce, but eventually do hit it when ports are down.",
          "author": "marting@netapp.com",
          "creation_time": "20120719T14:02:35",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120719T14:02:35",
          "id": "4909625",
          "is_private": "False"
        },
        {
          "count": "33",
          "creator": "ayyanar@netapp.com",
          "text": "Chad,\n\nGiven that we are hitting IO outages during fabric faults in our tests on RHEL6.3 QLogic FC, do you think the above QLogic command queuing problem is contributing to this? Have you identified a fix for this yet?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120720T10:50:10",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120720T10:50:10",
          "id": "4911732",
          "is_private": "False"
        },
        {
          "count": "34",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #32)\n> \n> Point is with the current default of 1024 max outstanding commands in the\n> ISP queue, we are hitting the problem consistently during straight IO itself\n> i.e. even before fabric faults are introduced. But after bumping this up to\n> a higher value like say 16384, the issue is harder to reproduce, but\n> eventually do hit it when ports are down.\n\nThis is a good data point to have.  There is a time when a port goes down and when we tell the FC transport that to block the port (essentially to start the dev_loss_timer) where we fill up with outstanding commands but obviously we're not going to get any response to those commands from a dead port so that might explain why you still see this with an value of 16384.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120720T20:02:26",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120720T20:02:26",
          "id": "4912763",
          "is_private": "False"
        },
        {
          "count": "35",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #33)\n> Chad,\n> \n> Given that we are hitting IO outages during fabric faults in our tests on\n> RHEL6.3 QLogic FC, do you think the above QLogic command queuing problem is\n> contributing to this?\n\nIt's probably contributing since once we're full we can't accept anymore commands until we start receiving responses to the outstanding ones.\n\n> Have you identified a fix for this yet?\n\nNo, not yet.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120720T20:04:19",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120720T20:04:19",
          "id": "4912766",
          "is_private": "False"
        },
        {
          "count": "36",
          "creator": "ayyanar@netapp.com",
          "text": "Marking the severity as 'Urgent' as this seems to contribute towards IO outages on the host.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120725T08:06:39",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120725T08:06:39",
          "id": "4919828",
          "is_private": "False"
        },
        {
          "count": "37",
          "attachment_id": "601537",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 601537\nqla2xxx: Determine the number of outstanding commands on a per adapter basis.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20120731T14:33:36",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120731T14:33:36",
          "id": "4931998",
          "is_private": "False"
        },
        {
          "count": "38",
          "creator": "cdupuis@redhat.com",
          "text": "NetApp, could you try the patch attached in comment 37?\n\nIt, as you proposed before, increases the size of the oustanding_cmds array but only to 2048 at this point.  We don't want to increase the size of this particular array since this memory will be allocated for the entire lifetime of the driver for each adapter.  However, since in previous testing it was shown that increasing the size of the array did help and further analysis of the initial dump taken showed that there can be a bursty behavior in how commands are submitted to the driver it seems to make sense to increase the capacity here.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120731T14:40:10",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120731T14:40:10",
          "id": "4932018",
          "is_private": "False"
        },
        {
          "count": "39",
          "attachment_id": "602524",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 602524\nmessages file with patch in comment 37",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120806T14:06:11",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120806T14:06:11",
          "id": "4942470",
          "is_private": "False"
        },
        {
          "count": "40",
          "creator": "ayyanar@netapp.com",
          "text": "Chad,\n\nRan test with patched kernel in comment #37 and message file attached.\n\nI am still seeing \u201cStart scsi failed\u201d messages only during FC port block test.\n\nAug  3 04:29:24 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:6: Start scsi failed rval=258 for cmd=ffff88028365e9c0.\nAug  3 11:59:24 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:5: Start scsi failed rval=258 for cmd=ffff880077b689c0.\n\nAug  3 04:28:24 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nAug  3 11:59:44 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120806T14:10:17",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120806T14:10:17",
          "id": "4942484",
          "is_private": "False"
        },
        {
          "count": "41",
          "creator": "cdupuis@redhat.com",
          "text": "The results at least are better.  In this latest test, when you see the start_scsi errors, does I/O eventually recover?",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120807T14:05:20",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120807T14:05:20",
          "id": "4946170",
          "is_private": "False"
        },
        {
          "count": "42",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to comment #41)\n> The results at least are better.  In this latest test, when you see the\n> start_scsi errors, does I/O eventually recover?\n\nYes IO recover but I am seeing IO outages and path validation failures.\n\nPath failures (path state remain as ''failed ready running'' after 10mins of fault recovery) are seen only on QLogic host.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120807T14:29:35",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120807T14:29:35",
          "id": "4946228",
          "is_private": "False"
        },
        {
          "count": "43",
          "creator": "marting@netapp.com",
          "text": "The behavior is definitely better with the patch from comment #37, but it does not seem to provide a complete fix for the issue. And as described by Ayyanar above, intermittent IO outages & path failures are still seen after applying this patch.\n\nSo how do we proceed here?",
          "author": "marting@netapp.com",
          "creation_time": "20120809T15:52:10",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120809T15:52:10",
          "id": "4956582",
          "is_private": "False"
        },
        {
          "count": "44",
          "creator": "cdupuis@redhat.com",
          "text": "Martin, I believe the other part of the equation here is to throttle down the queue depth of the attached devices when we essentially hit a host queue full condition.  lpfc and libfc have algorithms (of varying complexity) to ramp down the queue depth of attached devices when they run into a condition where the driver runs out of buffers to send commands.  This is the direction I believe we should head here.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120810T14:53:49",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120810T14:53:49",
          "id": "4959090",
          "is_private": "False"
        },
        {
          "count": "45",
          "creator": "marting@netapp.com",
          "text": "Chad,\n\nDo you have any updates on this?",
          "author": "marting@netapp.com",
          "creation_time": "20120821T08:13:03",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120821T08:13:03",
          "id": "4985101",
          "is_private": "False"
        },
        {
          "count": "46",
          "creator": "cdupuis@redhat.com",
          "text": "Hi Martin,\n\nStill working on this.  Should hopefully have a patch for you to test by the end of the week or early next week.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120822T13:24:00",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120822T13:24:00",
          "id": "4989018",
          "is_private": "False"
        },
        {
          "count": "47",
          "attachment_id": "607670",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 607670\nqla2xxx: BZ829739 Composite Patch 8/28/2012.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20120828T17:53:57",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120828T17:53:57",
          "id": "5002586",
          "is_private": "False"
        },
        {
          "count": "48",
          "creator": "cdupuis@redhat.com",
          "text": "Netapp, please try the patch from comment 47.  It still contains the changes that were in comment 37 but also adds:\n\n- A back-off algorithm when we run out of host resources.  This is done by throttling the queue depth of all devices attached to the adapter geometically (i.e. dividing by two), waiting for a settle period (60 seconds) and then slowly increasing the queue depth for each device on the adapter by 1 every 30 seconds until we hit ql2xmaxqdepth.\n\n- I've removed the ''fc_remote_port_chkready failed'' and ''Start scsi failed'' debug messages which are clogging up the system logs.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120828T17:59:24",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120828T17:59:24",
          "id": "5002595",
          "is_private": "False"
        },
        {
          "count": "49",
          "creator": "cdupuis@redhat.com",
          "text": "Any update on this; does this help in your testing?",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120911T14:04:02",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120911T14:04:02",
          "id": "5031966",
          "is_private": "False"
        },
        {
          "count": "50",
          "attachment_id": "611866",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 611866\nmessages file with patch in comment 47",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120911T16:49:18",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120911T16:49:18",
          "id": "5032503",
          "is_private": "False"
        },
        {
          "count": "51",
          "creator": "ayyanar@netapp.com",
          "text": ">Any update on this; does this help in your testing?\n\nSince ''Start scsi failed'' messages are not logged, I am not sure we are hitting those conditions or not.\n\nIO delay (no IO progress during FC port block) is seen with this patch too.\nWill update the status once I am done with few more testing.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120911T16:55:28",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120911T16:55:28",
          "id": "5032515",
          "is_private": "False"
        },
        {
          "count": "52",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #51)\n> >Any update on this; does this help in your testing?\n> \n> Since ''Start scsi failed'' messages are not logged, I am not sure we are\n> hitting those conditions or not.\n> \n> IO delay (no IO progress during FC port block) is seen with this patch too.\n> Will update the status once I am done with few more testing.\n\nThank you for the update Ayyanar.  For the most part you are not hitting the host exhaustion but it does occur a few times.  I saw the following statements:\n\nSep  2 20:13:50 ibmx3550-229-31 kernel: qla2xxx [0000:1a:00.0]-ffff:4: 4:0:0: Ramping down queue depth to 16\n\nwhich does mean we are hitting the condition occassionally.  The idea about ramping down the queue depth was to allow the host some time to recover before resuming full throttle i/o.  Are your i/o outages any shorter?  If they are not then the whole ramp up/ramp down thing may not be worth the effort.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120911T17:11:40",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120911T17:11:40",
          "id": "5032571",
          "is_private": "False"
        },
        {
          "count": "53",
          "creator": "ayyanar@netapp.com",
          "text": " Are your i/o outages any shorter?  If they are not then the whole ramp up/ramp down thing may not be worth the effort.\n\nYes. IO outages are shorter now (less than 180 sec). With the last run I hit mailbox command timed out issue, and firmware dump is cleared.\n\nkernel: qla2xxx [0000:1a:00.1]-d001:2: Firmware dump saved to temp buffer (2/ffffc9001286a000).\nkernel: qla2xxx [0000:1a:00.1]-101e:2: Mailbox cmd timeout occured, cmd=0x54, mb[0]=0x54. Scheduling ISP abort\nkernel: qla2xxx [0000:1a:00.1]-00af:2: Performing ISP error recovery - ha=ffff880037464000.\nkernel: qla2xxx [0000:1a:00.1]-4801:2: DPC handler waking up.\nkernel: qla2xxx [0000:1a:00.1]-4802:2: dpc_flags=0x8.\nkernel: qla2xxx [0000:1a:00.1]-4800:2: DPC handler sleeping.\nkernel: qla2xxx [0000:1a:00.1]-705e:2: Raw firmware dump ready for read on (2).\nkernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120912T11:01:12",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120912T11:01:12",
          "id": "5034366",
          "is_private": "False"
        },
        {
          "count": "54",
          "attachment_id": "612059",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 612059\nmessages file with patch in comment 47-mailbox cmd timeout",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20120912T11:02:14",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120912T11:02:14",
          "id": "5034369",
          "is_private": "False"
        },
        {
          "count": "55",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #53)\n>  Are your i/o outages any shorter?  If they are not then the whole ramp\n> up/ramp down thing may not be worth the effort.\n> \n> Yes. IO outages are shorter now (less than 180 sec). With the last run I hit\n> mailbox command timed out issue, and firmware dump is cleared.\n> \n> kernel: qla2xxx [0000:1a:00.1]-d001:2: Firmware dump saved to temp buffer\n> (2/ffffc9001286a000).\n> kernel: qla2xxx [0000:1a:00.1]-101e:2: Mailbox cmd timeout occured,\n> cmd=0x54, mb[0]=0x54. Scheduling ISP abort\n> kernel: qla2xxx [0000:1a:00.1]-00af:2: Performing ISP error recovery -\n> ha=ffff880037464000.\n> kernel: qla2xxx [0000:1a:00.1]-4801:2: DPC handler waking up.\n> kernel: qla2xxx [0000:1a:00.1]-4802:2: dpc_flags=0x8.\n> kernel: qla2xxx [0000:1a:00.1]-4800:2: DPC handler sleeping.\n> kernel: qla2xxx [0000:1a:00.1]-705e:2: Raw firmware dump ready for read on\n> (2).\n> kernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).\n\nDo you have the firmware dump?  Would be good for analysis.  Looks like it occurred after a switch port disable during the latter half of a fabric rescan.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20120914T13:21:01",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20120914T13:21:01",
          "id": "5040719",
          "is_private": "False"
        },
        {
          "count": "56",
          "creator": "ayyanar@netapp.com",
          "text": "Do you have the firmware dump?\n\nNo Chad. It is cleared.\n\nkernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).\n\nexcept, this one instance I have not seen firmware dump.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20120921T10:42:27",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20120921T10:42:27",
          "id": "5056007",
          "is_private": "False"
        },
        {
          "count": "57",
          "attachment_id": "615414",
          "author": "rajashekhar.a@netapp.com",
          "text": "Created attachment 615414\nsyslog with patch from comment 47\n\nI tried the patch in comment 47 and it seems to have worked. I am not seeing the messages and also paths come up fine within expected time.\n\nCould you please push this fix to the upcoming errata? This fixes a couple of issues that we are hitting.",
          "creator": "rajashekhar.a@netapp.com",
          "creation_time": "20120921T12:48:37",
          "bug_id": "829739",
          "creator_id": "210755",
          "time": "20120921T12:48:37",
          "id": "5056286",
          "is_private": "False"
        },
        {
          "count": "58",
          "creator": "marting@netapp.com",
          "text": "So looks like the patch from comment #47 has indeed helped improve the behavior here on QLogic FC hosts.\n\nRequesting a z-stream errata fix for the same since this involves path failures and long IO outages on the host.",
          "author": "marting@netapp.com",
          "creation_time": "20120924T11:41:11",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20120924T11:41:11",
          "id": "5060394",
          "is_private": "False"
        },
        {
          "count": "59",
          "attachment_id": "629442",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 629442\nComposite patch 10/18/2012.\n\nLatest version of the patch(es) to handle host resource exhaustion.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20121018T14:03:08",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121018T14:03:08",
          "id": "5126046",
          "is_private": "False"
        },
        {
          "count": "60",
          "creator": "ayyanar@netapp.com",
          "text": "Ran test with your latest patch from comment#59 and still seeing IO outages and attached log.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20121101T08:56:07",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20121101T08:56:07",
          "id": "5162474",
          "is_private": "False"
        },
        {
          "count": "61",
          "attachment_id": "636485",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 636485\nmessages file with patch in comment 59",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20121101T08:59:40",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20121101T08:59:40",
          "id": "5162479",
          "is_private": "False"
        },
        {
          "count": "62",
          "creator": "ayyanar@netapp.com",
          "text": "''Start scsi failed messages'' are not seen from logs. But Queue depth adjusting messages and Abort messages are there.\n\nkernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for nexus=1:0:16.\nkernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for nexus=1:0:17.\n\nkernel: qla2xxx [0000:0c:00.1]-8802:1: Aborting from RISC nexus=1:0:1 sp=ffff881047f36680 cmd=ffff88104671c3c0\nkernel: qla2xxx [0000:0c:00.1]-3822:1: FCP command status: 0x5-0x0 (0x80000) nexus=1:0:1 portid=015000 oxid=0x4a2 cdb=2a00007c2a0000008000 len=0x10000 rsp_info=0x0 resid=0x0 fw_resid=0x0.\nkernel: qla2xxx [0000:0c:00.1]-8804:1: Abort command mbx success cmd=ffff88104671c3c0.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20121102T10:21:01",
          "bug_id": "829739",
          "creator_id": "317542",
          "time": "20121102T10:21:01",
          "id": "5165735",
          "is_private": "False"
        },
        {
          "count": "63",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #62)\n> ''Start scsi failed messages'' are not seen from logs. But Queue depth\n> adjusting messages and Abort messages are there.\n> \n> kernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for\n> nexus=1:0:16.\n> kernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for\n> nexus=1:0:17.\n> \n> kernel: qla2xxx [0000:0c:00.1]-8802:1: Aborting from RISC nexus=1:0:1\n> sp=ffff881047f36680 cmd=ffff88104671c3c0\n> kernel: qla2xxx [0000:0c:00.1]-3822:1: FCP command status: 0x5-0x0 (0x80000)\n> nexus=1:0:1 portid=015000 oxid=0x4a2 cdb=2a00007c2a0000008000 len=0x10000\n> rsp_info=0x0 resid=0x0 fw_resid=0x0.\n> kernel: qla2xxx [0000:0c:00.1]-8804:1: Abort command mbx success\n> cmd=ffff88104671c3c0.\n\nThe ''Queue depth adjusted-down'' messages from this log are actually the result of queue fulls being reported by the target:\n\nNov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE FULL detected. \n\nI did not see any messages that pointed to resource exhaustion on the driver side.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20121102T13:21:22",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121102T13:21:22",
          "id": "5166062",
          "is_private": "False"
        },
        {
          "count": "64",
          "creator": "marting@netapp.com",
          "text": "(In reply to comment #63)\n> The ''Queue depth adjusted-down'' messages from this log are actually the\n> result of queue fulls being reported by the target:\n> \n> Nov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE\n> FULL detected. \n> \n> I did not see any messages that pointed to resource exhaustion on the driver\n> side.\n\nJust curious. When hitting a QUEUE FULL scenario, the target expects the host to back off and wait for at least one command to return before resuming IO. Is that expectation met with this patchset?",
          "author": "marting@netapp.com",
          "creation_time": "20121106T11:11:12",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20121106T11:11:12",
          "id": "5174073",
          "is_private": "False"
        },
        {
          "count": "65",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #64)\n> (In reply to comment #63)\n> > The ''Queue depth adjusted-down'' messages from this log are actually the\n> > result of queue fulls being reported by the target:\n> > \n> > Nov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE\n> > FULL detected. \n> > \n> > I did not see any messages that pointed to resource exhaustion on the driver\n> > side.\n> \n> Just curious. When hitting a QUEUE FULL scenario, the target expects the\n> host to back off and wait for at least one command to return before resuming\n> IO. Is that expectation met with this patchset?\n\nThe target QUEUE FULL scenario is actually controlled by the SCSI layer in RHEL 6.  The qla2xxx driver simply returns the queue full status back to the SCSI layer and it is there that the queue depth is ramped down.  From what I can see in the SCSI layer, the host will ramp down it's queue depth but will not completely block.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20121106T14:42:23",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121106T14:42:23",
          "id": "5174665",
          "is_private": "False"
        },
        {
          "count": "66",
          "creator": "marting@netapp.com",
          "text": "Chad,\n\nIgnore comment #60 - the IO outages there seem to be caused by a target bug (that causes frames to be dropped), and nothing to do with your patchset.\n\nAnd we've now run extensive tests with your latest patchset provided in comment #59. And the results look good - no longer seeing the above resource exhaustion issues on the QLogic host.\n\nSo requesting you to include this patchset now in the inbox qla2xxx driver.",
          "author": "marting@netapp.com",
          "creation_time": "20121119T19:00:52",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20121119T19:00:52",
          "id": "5211939",
          "is_private": "False"
        },
        {
          "count": "67",
          "creator": "marting@netapp.com",
          "text": "Chris,\n\nWithout this fix, the customer would end up seeing path failures & occasional long IO outages during fabric faults. So requesting for a z-stream 6.3.z errata fix for the same.\n\nThanks,\n-Martin",
          "author": "marting@netapp.com",
          "creation_time": "20121120T03:57:43",
          "bug_id": "829739",
          "creator_id": "207401",
          "time": "20121120T03:57:43",
          "id": "5213066",
          "is_private": "False"
        },
        {
          "count": "68",
          "creator": "ctatman@redhat.com",
          "text": "Thanks Martin,\n\nThe zstream request process has been initiated and the flags are all set.\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121120T04:54:41",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121120T04:54:41",
          "id": "5213152",
          "is_private": "False"
        },
        {
          "count": "69",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nI see that the fix has been posted for review.  Has it actually been accepted into 6.4 yet?  Martin has stated their testing with 6.4 alpha builds is still failing.\n\nJust trying to figure out when/where we can expect a package that we can test.\n\nThanks!\n\n--Chrisa",
          "author": "ctatman@redhat.com",
          "creation_time": "20121128T19:21:54",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121128T19:21:54",
          "id": "5237271",
          "is_private": "False"
        },
        {
          "count": "70",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to comment #69)\n> Hi Chad,\n> \n> I see that the fix has been posted for review.  Has it actually been\n> accepted into 6.4 yet?\n\nNot yet, it's going through the review process.\n\n>  Martin has stated their testing with 6.4 alpha\n> builds is still failing.\n> \n> Just trying to figure out when/where we can expect a package that we can\n> test.\n> \n> Thanks!\n> \n> --Chrisa",
          "author": "cdupuis@redhat.com",
          "creation_time": "20121128T21:43:40",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121128T21:43:40",
          "id": "5238211",
          "is_private": "False"
        },
        {
          "count": "71",
          "creator": "jpallich@redhat.com",
          "text": "This bug has been copied as 6.3 z-stream (EUS) bug #882205 and now must be\nresolved in the current update release, set blocker flag.\n\nCopy created by jpallich@redhat.com.",
          "author": "jpallich@redhat.com",
          "creation_time": "20121130T11:48:08",
          "bug_id": "829739",
          "creator_id": "314851",
          "time": "20121130T11:48:08",
          "id": "5245132",
          "is_private": "True"
        },
        {
          "count": "72",
          "creator": "ctatman@redhat.com",
          "text": "Can anyone tell me if this made the kernel 7 cutoff date?  Or is this going to be in kernel 8?\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121205T20:27:27",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121205T20:27:27",
          "id": "5261628",
          "is_private": "True"
        },
        {
          "count": "73",
          "creator": "revers@redhat.com",
          "text": "Unless I'm mistaken, this still isn't posted upstream and that is blocking acceptance into rhel6.4.",
          "author": "revers@redhat.com",
          "creation_time": "20121205T21:55:36",
          "bug_id": "829739",
          "creator_id": "272069",
          "time": "20121205T21:55:36",
          "id": "5261826",
          "is_private": "False"
        },
        {
          "count": "74",
          "creator": "ctatman@redhat.com",
          "text": "Rob,\n\nWho is responsible for posting this upstream?\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121206T13:57:14",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121206T13:57:14",
          "id": "5264231",
          "is_private": "False"
        },
        {
          "count": "75",
          "creator": "ctatman@redhat.com",
          "text": "Just saw this email response from Chad this morning:\n\n''The issue has been with upstream posting.  We plan on submitting this to linux-scsi tomorrow.''\n\nReminder:\nWe have missed the kernel 7 deadline and we have only one zstream kernel left to us for 6.3.  \n\nThe deadline for patch acceptance for Kernel 8 for 6.3 is: 1/09/2012, so we have a little time left to us.  But this patch needs to be accepted upstream before we can complete the post review on our end and get this into 6.4 and 6.3.z.\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121206T14:08:08",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121206T14:08:08",
          "id": "5264270",
          "is_private": "False"
        },
        {
          "count": "76",
          "creator": "ctatman@redhat.com",
          "text": "Chad,\n\nSetting this bug to waiting on you while we are waiting for the upstream ack on the patch.\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121206T14:09:23",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121206T14:09:23",
          "id": "5264278",
          "is_private": "False"
        },
        {
          "count": "77",
          "creator": "cdupuis@redhat.com",
          "text": "Upstream postings:\n\nqla2xxx: Determine the number of outstanding commands based on available resources.\nhttp://marc.info/?l=linux-scsi&m=135486817323393&w=2\n\nqla2xxx: Ramp down queue depth for attached SCSI devices when driver resources are low.\nhttp://marc.info/?l=linux-scsi&m=135486658422862&w=2",
          "author": "cdupuis@redhat.com",
          "creation_time": "20121207T13:41:01",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121207T13:41:01",
          "id": "5267724",
          "is_private": "False"
        },
        {
          "count": "78",
          "creator": "cdupuis@redhat.com",
          "text": "FYI, these two patches have been accepted upstream by the SCSI maintainer.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20121219T21:50:21",
          "bug_id": "829739",
          "creator_id": "295897",
          "time": "20121219T21:50:21",
          "id": "5304047",
          "is_private": "False"
        },
        {
          "count": "79",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nThanks for the update.  Can you give us a guess as to which snap this is going to make it into for 6.4?\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20121220T14:15:08",
          "bug_id": "829739",
          "creator_id": "161143",
          "time": "20121220T14:15:08",
          "id": "5305927",
          "is_private": "True"
        },
        {
          "count": "80",
          "creator": "jarod@redhat.com",
          "text": "Patch(es)",
          "author": "jarod@redhat.com",
          "creation_time": "20121220T21:22:28",
          "bug_id": "829739",
          "creator_id": "19949",
          "time": "20121220T21:22:28",
          "id": "5307848",
          "is_private": "False"
        },
        {
          "count": "81",
          "creator": "jarod@redhat.com",
          "text": "List\nRelated patch: http://patchwork.lab.bos.redhat.com/patch/55633\nRelated patch: http://patchwork.lab.bos.redhat.com/patch/55834",
          "author": "jarod@redhat.com",
          "creation_time": "20121220T21:26:57",
          "bug_id": "829739",
          "creator_id": "19949",
          "time": "20121220T21:26:57",
          "id": "5307886",
          "is_private": "True"
        },
        {
          "count": "82",
          "creator": "errata-xmlrpc@redhat.com",
          "text": "Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHBA-2012:13387-01\nhttp://errata.devel.redhat.com/errata/show/13387",
          "author": "errata-xmlrpc@redhat.com",
          "creation_time": "20121220T21:32:23",
          "bug_id": "829739",
          "creator_id": "241731",
          "time": "20121220T21:32:23",
          "id": "5307925",
          "is_private": "True"
        },
        {
          "count": "83",
          "creator": "jpallich@redhat.com",
          "text": "This bug has been copied as 6.3 z-stream (EUS) bug #891564 and now must be\nresolved in the current update release, set blocker flag.\n\nCopy created by jpallich@redhat.com.",
          "author": "jpallich@redhat.com",
          "creation_time": "20130103T09:59:44",
          "bug_id": "829739",
          "creator_id": "314851",
          "time": "20130103T09:59:44",
          "id": "5324349",
          "is_private": "True"
        },
        {
          "count": "84",
          "creator": "bmarson@redhat.com",
          "text": "While working another issue with gfs engineering, I ran a kernel with the driver/firmware changes associated with this bz and tested a grid based SAS (ISV) application on a 4 node cluster.  Card information is:\n\ndriver_version: 8.04.00.08.06.4-k\nfw_version:     5.08.00 (9496)\nmodel_desc:     PCI-Express Dual Port 4Gb Fibre Channel HBA\nmodel_name:     HPAE312A\n\n# lspci | grep -i logic\n0d:00.0 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)\n0d:00.1 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)\n\nThere was no performance regression with my tests, but I'm concerned with the volume of messages regarding 'Ramping down queue depths'.  My test configuration has 192 multipaths of which half are typically active.  During my 2.5 hour test run which generates around 2TB of I/O (45%reads 55% writes), each node logged around 10K messages.\n\nBarry",
          "author": "bmarson@redhat.com",
          "creation_time": "20130107T21:44:52",
          "bug_id": "829739",
          "creator_id": "188487",
          "time": "20130107T21:44:52",
          "id": "5335521",
          "is_private": "False"
        },
        {
          "count": "85",
          "creator": "revers@redhat.com",
          "text": "The print output in question is:\n\nJan  7 12:08:23 pats kernel: qla2xxx [0000:0d:00.1]-3031:2: 2:7:12: Ramping down queue depth to 3\n\nBarry,\n\nCan you open another bug due to the prints here?\n\nThanks, Rob",
          "author": "revers@redhat.com",
          "creation_time": "20130108T15:38:16",
          "bug_id": "829739",
          "creator_id": "272069",
          "time": "20130108T15:38:16",
          "id": "5339722",
          "is_private": "False"
        },
        {
          "count": "86",
          "creator": "bdonahue@redhat.com",
          "text": "You want me to add a BZ because this message shouldn't be there?",
          "author": "bdonahue@redhat.com",
          "creation_time": "20130108T16:05:53",
          "bug_id": "829739",
          "creator_id": "197259",
          "time": "20130108T16:05:53",
          "id": "5339799",
          "is_private": "True"
        },
        {
          "count": "87",
          "creator": "revers@redhat.com",
          "text": "(In reply to comment #86)\n> You want me to add a BZ because this message shouldn't be there?\n\nno, this was to barry marson :)",
          "author": "revers@redhat.com",
          "creation_time": "20130109T17:45:38",
          "bug_id": "829739",
          "creator_id": "272069",
          "time": "20130109T17:45:38",
          "id": "5345402",
          "is_private": "True"
        },
        {
          "count": "88",
          "creator": "revers@redhat.com",
          "text": "Other performance data indicated a significant regression with this update.  The patches for this bugzilla are going to be reverted for rhel6.4.",
          "author": "revers@redhat.com",
          "creation_time": "20130109T17:47:22",
          "bug_id": "829739",
          "creator_id": "272069",
          "time": "20130109T17:47:22",
          "id": "5345427",
          "is_private": "False"
        },
        {
          "count": "89",
          "creator": "fge@redhat.com",
          "text": "(In reply to comment #88)\n> Other performance data indicated a significant regression with this update. \n> The patches for this bugzilla are going to be reverted for rhel6.4.\n\nRob,\nSince this bug is ON_QA, does it mean patch reverted or still on going?",
          "author": "fge@redhat.com",
          "creation_time": "20130116T04:17:16",
          "bug_id": "829739",
          "creator_id": "303525",
          "time": "20130116T04:17:16",
          "id": "5367166",
          "is_private": "False"
        },
        {
          "count": "90",
          "creator": "revers@redhat.com",
          "text": "(In reply to comment #89)\n> (In reply to comment #88)\n> > Other performance data indicated a significant regression with this update. \n> > The patches for this bugzilla are going to be reverted for rhel6.4.\n> \n> Rob,\n> Since this bug is ON_QA, does it mean patch reverted or still on going?\n\nWe are still waiting for some more performance data, and the revert is actually\non hold.  I suggest continuing as if this were being included for now.",
          "author": "revers@redhat.com",
          "creation_time": "20130116T16:09:27",
          "bug_id": "829739",
          "creator_id": "272069",
          "time": "20130116T16:09:27",
          "id": "5373592",
          "is_private": "False"
        },
        {
          "count": "91",
          "creator": "fge@redhat.com",
          "text": "We havn't install the 8G FC qla25xx card into your test environment.\n\nRegression pass for 16G FC qla26xx card on kernel -358.\n\nhttps://beaker.engineering.redhat.com/jobs/375458\n\nSanityOnly.",
          "author": "fge@redhat.com",
          "creation_time": "20130208T02:41:42",
          "bug_id": "829739",
          "creator_id": "303525",
          "time": "20130208T02:41:42",
          "id": "5488369",
          "is_private": "False"
        },
        {
          "count": "92",
          "creator": "errata-xmlrpc@redhat.com",
          "text": "Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHSA-2012:13387-06 has been changed to PUSH_READY status.\nhttp://errata.devel.redhat.com/errata/show/13387",
          "author": "errata-xmlrpc@redhat.com",
          "creation_time": "20130220T15:32:56",
          "bug_id": "829739",
          "creator_id": "241731",
          "time": "20130220T15:32:56",
          "id": "5523386",
          "is_private": "True"
        },
        {
          "count": "93",
          "creator": "errata-xmlrpc@redhat.com",
          "text": "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttp://rhn.redhat.com/errata/RHSA-2013-0496.html",
          "author": "errata-xmlrpc@redhat.com",
          "creation_time": "20130221T06:21:09",
          "bug_id": "829739",
          "creator_id": "241731",
          "time": "20130221T06:21:09",
          "id": "5526287",
          "is_private": "False"
        }
      ],
      "cf_show_homepage": "---",
      "platform": "All",
      "version": [
        "6.3"
      ],
      "cc": [
        "andrew.vasquez@qlogic.com",
        "arun.easi@qlogic.com",
        "bdonahue@redhat.com",
        "bmarson@redhat.com",
        "cdupuis@redhat.com",
        "coughlan@redhat.com",
        "ctatman@redhat.com",
        "czhang@redhat.com",
        "dhoward@redhat.com",
        "fge@redhat.com",
        "ichute@redhat.com",
        "jwest@redhat.com",
        "marting@netapp.com",
        "mchristi@redhat.com",
        "msnitzer@redhat.com",
        "revers@redhat.com",
        "salmy@redhat.com",
        "xdl-redhat-bugzilla@netapp.com"
      ],
      "cf_verified": [
        "SanityOnly"
      ],
      "cf_cust_facing": "---",
      "cf_environment": "",
      "status": "CLOSED",
      "classification": "Red Hat",
      "cf_verified_branch": "",
      "blocks": [
        "724056",
        "1033136",
        "786478",
        "846704",
        "882205",
        "891564"
      ],
      "qa_contact": "fge@redhat.com",
      "tags": [],
      "see_also": [],
      "component": [
        "kernel"
      ],
      "remaining_time": "0.0",
      "sub_components": {},
      "cf_pgm_internal": "",
      "cf_doc_type": "Bug Fix",
      "cf_clone_of": "0",
      "groups": [
        "devel",
        "netapp",
        "qa",
        "qlogic",
        "redhat"
      ],
      "cf_documentation_action": "---",
      "cf_internal_whiteboard": "ptam GSSApproved",
      "target_milestone": "rc",
      "cf_devel_whiteboard": "",
      "is_cc_accessible": "True",
      "cf_type": "Bug",
      "cf_category": "---",
      "url": "",
      "cf_build_id": "",
      "whiteboard": "",
      "cf_crm": "",
      "summary": "[NetApp 6.3 Bug] QLogic 'Start scsi failed' error messages seen on 8G FC host during IO with fabric faults",
      "alias": [],
      "op_sys": "Linux",
      "flags": [
        {
          "status": "?",
          "name": "qe_test_coverage",
          "modification_date": "20120925T22:13:25",
          "type_id": "318",
          "is_active": "1",
          "creation_date": "20120925T22:13:25",
          "id": "1150021",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "requires_doc_text",
          "modification_date": "20121211T14:47:52",
          "type_id": "415",
          "is_active": "1",
          "creation_date": "20121211T14:47:52",
          "id": "1212413",
          "setter": "bugzilla@redhat.com"
        },
        {
          "status": "+",
          "name": "rhel-6.4.0",
          "modification_date": "20120925T22:03:50",
          "type_id": "326",
          "is_active": "0",
          "creation_date": "20120607T13:22:19",
          "id": "1066986",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "blocker",
          "modification_date": "20121130T11:48:19",
          "type_id": "24",
          "is_active": "1",
          "creation_date": "20121130T11:48:19",
          "id": "1202571",
          "setter": "jpallich@redhat.com"
        },
        {
          "status": "+",
          "name": "pm_ack",
          "modification_date": "20120627T13:47:33",
          "type_id": "11",
          "is_active": "1",
          "creation_date": "20120607T13:36:37",
          "id": "1067003",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "devel_ack",
          "modification_date": "20120620T20:46:08",
          "type_id": "10",
          "is_active": "1",
          "creation_date": "20120608T17:28:08",
          "id": "1068208",
          "setter": "coughlan@redhat.com"
        },
        {
          "status": "+",
          "name": "qa_ack",
          "modification_date": "20120925T21:59:05",
          "type_id": "9",
          "is_active": "1",
          "creation_date": "20120608T17:28:08",
          "id": "1068209",
          "setter": "bdonahue@redhat.com"
        }
      ],
      "last_change_time": "20131123T01:53:57",
      "assigned_to": "cdupuis@redhat.com",
      "update_token": "1403125936-ZHIcJ2GOJxKtvp1kOn265ZAPDPPq8Xy0Swp7xICT8nU",
      "cf_partner": [
        "NetApp",
        "Qlogic"
      ],
      "cf_last_closed": "20130221T06:21:09",
      "resolution": "ERRATA",
      "cf_mount_type": "---",
      "cf_layered_products": []
    },
    {
      "cf_qe_conditional_nak": [],
      "classification": "Red Hat",
      "cf_pm_score": "1800",
      "estimated_time": "0.0",
      "depends_on": [
        "1054299",
        "1054301",
        "1076497",
        "829739"
      ],
      "cf_story_points": "---",
      "cf_conditional_nak": [],
      "creation_time": "20131121T15:37:00",
      "actual_time": "0.0",
      "docs_contact": "",
      "is_open": "True",
      "keywords": [],
      "summary": "[NetApp 6.3 Bug] QLogic 'Start scsi failed' error messages seen on 8G FC host during IO with fabric faults",
      "external_bugs": [],
      "id": "1033136",
      "cf_release_notes": "",
      "priority": "urgent",
      "severity": "urgent",
      "is_confirmed": "True",
      "is_creator_accessible": "True",
      "cf_fixed_in": "",
      "creator": "salmy@redhat.com",
      "comments": [
        {
          "count": "0",
          "creator": "salmy@redhat.com",
          "text": "+++ This bug was initially created as a clone of Bug #829739 +++\n\nDescription of problem:\n\nDuring IO with fabric faults, one generally sees several \"kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff88028ea52980\" messages in the syslog as shown below when QLogic driver verbosity is set to 0x1e400000:\n\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff880076ba1580.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff8800769cb4c0.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff8800769cb5c0.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff88007713b280.\nJun  7 17:29:01 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:1: Start scsi failed rval=258 for cmd=ffff880076ba1580.\nJun  7 17:29:01 ibmx3650-210-104 kernel: Start scsi failed rval=258 for cmd=ffff8800769cb2c0.\n\nVersion-Release number of selected component (if applicable):\n\nRHEL6U3 alpha kernel onwards.\n\nmodel description: QLogic QLE2562\ndriver version:    v.8.04.00.04.06.3-k-debug\nfirmware version:  v.5.06.05 (90d5)\n\nHow reproducible:\n\nAlways.\n\nSteps to Reproduce:\n\n1.Map 20*10G (4path each)LUN's from NetApp array to QLogic FC host.\n2.Set QLogic driver verbosity to 0x1e400000\n3.Create PV on multipath device and 3 LV's of 100G each.\n4.Create ext4 FS on LV's and mount them.\n5.Start IO on mount points and then introduce storage fault.\n\n--- Additional comment from RHEL Product and Program Management on 2012-06-07 09:22:24 EDT ---\n\nSince this issue was entered in bugzilla, the release flag has been\nset to ? to ensure that it is properly evaluated for this release.\n\n--- Additional comment from Martin George on 2012-06-07 10:06:37 EDT ---\n\nThe above 'Start scsi failed' error messages are seen consistently during perturbations in our tests. As per Chad, these indicate an error when queuing a request on the request queue and should not be seen.\n\nSecondly, with the standard QLogic logging (0x1e400000) enabled, several 'fc_remote_port_chkready failed' messages are also seen as shown below:\n\nkernel: qla2xxx [0000:1a:00.0]-3803:6: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\nkernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\nkernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for cmd=ffff8800781a7680, rval=0xf0000.\n\nThese as per Chad are benign, and is only an indication that the queuecommand handler is called when the rport gets blocked. But since these flood the /var/log/messages which may cause the host to be sluggish, I would request QLogic to log these messages only with higher logging enabled i.e. like say 0x7fffffff.\n\n--- Additional comment from Chad Dupuis on 2012-06-11 10:25:33 EDT ---\n\n(In reply to comment #2)\n> The above 'Start scsi failed' error messages are seen consistently during\n> perturbations in our tests. As per Chad, these indicate an error when\n> queuing a request on the request queue and should not be seen.\n\nI'll create a debug patch to print where in qla24xx_start_scsi we're failing so that we can trace where the actual failure condition is.\n\n> \n> Secondly, with the standard QLogic logging (0x1e400000) enabled, several\n> 'fc_remote_port_chkready failed' messages are also seen as shown below:\n> \n> kernel: qla2xxx [0000:1a:00.0]-3803:6: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> kernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> kernel: qla2xxx [0000:1a:00.1]-3803:7: fc_remote_port_chkready failed for\n> cmd=ffff8800781a7680, rval=0xf0000.\n> \n> These as per Chad are benign, and is only an indication that the\n> queuecommand handler is called when the rport gets blocked. But since these\n> flood the /var/log/messages which may cause the host to be sluggish, I would\n> request QLogic to log these messages only with higher logging enabled i.e.\n> like say 0x7fffffff.\n\nWould it be possible to open a separate bugzilla for this message if only so that we don't confuse the two issues?\n\n--- Additional comment from Ayyanar on 2012-06-13 04:37:07 EDT ---\n\n> I'll create a debug patch to print where in qla24xx_start_scsi we're failing\n> so that we can trace where the actual failure condition is.\n\nCan you give this patch?\n\n> > Secondly, with the standard QLogic logging (0x1e400000) enabled, several\n> > 'fc_remote_port_chkready failed' messages are also seen as shown below:\n\n> Would it be possible to open a separate bugzilla for this message if only so\n> that we don't confuse the two issues?\n\nSure.Will do.\n\n--- Additional comment from Ayyanar on 2012-06-15 04:21:01 EDT ---\n\n\n> > Would it be possible to open a separate bugzilla for this message if only so\n> > that we don't confuse the two issues?\n\nFiled a separate bug 831529 for this. Is this patch ready?\n\n--- Additional comment from Chad Dupuis on 2012-06-15 13:55:09 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-06-15 13:55:47 EDT ---\n\n(In reply to comment #5)\n> > > Would it be possible to open a separate bugzilla for this message if only so\n> > > that we don't confuse the two issues?\n> \n> Filed a separate bug 831529 for this. Is this patch ready?\n\nYes, patch is the attachment from Comment 6.\n\n--- Additional comment from Ayyanar on 2012-06-18 08:59:56 EDT ---\n\nRan test with patched kernel in comment 6 and message file is attached.\n\nSoon after IO started, I am seeing below messages:\n\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No room left in outstanding commands array, index=1024.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4: Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3031:5: No room left in outstanding commands array, index=1024.\n\nFew kernel hung_task_timeout_secs also seen with dt.stable. Still continuing my test.\n\n--- Additional comment from Ayyanar on 2012-06-18 09:01:21 EDT ---\n\n\n\n--- Additional comment from Ayyanar on 2012-06-19 10:03:46 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-06-19 15:39:00 EDT ---\n\nJust curious looks like we start getting the messages:\n\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No room left in outstanding commands array, index=1024.\nJun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4: Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\n\neven before we start any type of perturbations?\n\n--- Additional comment from Ayyanar on 2012-06-20 05:22:35 EDT ---\n\n(In reply to comment #11)\n> Just curious looks like we start getting the messages:\n> \n> Jun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3031:4: No\n> room left in outstanding commands array, index=1024.\n> Jun 18 13:25:47 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:4:\n> Start scsi failed rval=258 for cmd=ffff8802f3f9e180.\n> \n> even before we start any type of perturbations?\n\nYes. Once IO started, we are seeing above messages.\n\n--- Additional comment from Martin George on 2012-06-25 07:58:52 EDT ---\n\nChad,\n\nAny updates here? \n\nGiven that we usually run heavy IO in our tests here, does this mean that the QLogic default of 1024 max outstanding commands in the ISP queue is insufficient for such scenarios, and this probably needs to be bumped up?\n\n--- Additional comment from RHEL Product and Program Management on 2012-06-27 09:47:36 EDT ---\n\nThis request was evaluated by Red Hat Product Management for\ninclusion in a Red Hat Enterprise Linux release.  Product\nManagement has requested further review of this request by\nRed Hat Engineering, for potential inclusion in a Red Hat\nEnterprise Linux release for currently deployed products.\nThis request is not yet committed for inclusion in a release.\n\n--- Additional comment from Chad Dupuis on 2012-06-28 17:25:12 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-06-28 17:27:52 EDT ---\n\n(In reply to comment #13)\n> Chad,\n> \n> Any updates here? \n> \n> Given that we usually run heavy IO in our tests here, does this mean that\n> the QLogic default of 1024 max outstanding commands in the ISP queue is\n> insufficient for such scenarios, and this probably needs to be bumped up?\n\nThat's a possibility Martin, but first we need to see what the contents of the oustanding_cmds array are to see what type of commands are filling it up.  The easiest way to do that at this point is to basically panic the system when this condition occurs to generate a vmcore that we can analyze.  If this is possible, I've attached a patch in comment 15 that will panic the system in the right spot.\n\nLet me know if this is doable.\n\n--- Additional comment from Ayyanar on 2012-07-03 11:21:35 EDT ---\n\nRan with the patch given on comment 5 and hit panic. \n\nKernel panic - not syncing: qla2xxx: oustanding_cmds array on scsi_host 2 is full\n\nPid: 37, comm: kblockd/3 Tainted: G           ---------------  T 2.6.32-279.el6.panic.bz829739.x86_64 #1\nCall Trace:\n [<ffffffff814fd11a>] ? panic+0xa0/0x168\n [<ffffffff811625e0>] ? cache_alloc_refill+0x1c0/0x240\n [<ffffffffa01f4f16>] ? qla24xx_start_scsi+0x4e6/0x650 [qla2xxx]\n [<ffffffff81116855>] ? mempool_alloc_slab+0x15/0x20\n [<ffffffffa01f5876>] ? qla24xx_dif_start_scsi+0x7f6/0x13f0 [qla2xxx]\n [<ffffffff812815a0>] ? sg_init_table+0x30/0x50\n [<ffffffff8128163e>] ? __sg_alloc_table+0x7e/0x130\n [<ffffffff8136b5e0>] ? scsi_sg_alloc+0x0/0x60\n [<ffffffff81116855>] ? mempool_alloc_slab+0x15/0x20\n [<ffffffff81116963>] ? mempool_alloc+0x63/0x140\n [<ffffffff8136b9f9>] ? scsi_setup_fs_cmnd+0x79/0xe0\n [<ffffffff81363180>] ? scsi_done+0x0/0x60\n [<ffffffffa01dbdbc>] ? qla2xxx_queuecommand+0x2fc/0x370 [qla2xxx]\n [<ffffffff81363411>] ? scsi_dispatch_cmd+0x101/0x360\n [<ffffffff8136af0d>] ? scsi_request_fn+0x41d/0x790\n [<ffffffff81253de3>] ? ftrace_raw_event_id_block_rq+0x153/0x190\n [<ffffffff812554c1>] ? __blk_run_queue+0x31/0x40\n [<ffffffff8124f6f8>] ? elv_insert+0xf8/0x1a0\n [<ffffffff8124f7ea>] ? __elv_add_request+0x4a/0x90\n [<ffffffff81254ead>] ? blk_insert_cloned_request+0x7d/0xc0\n [<ffffffffa00023ac>] ? dm_dispatch_request+0x3c/0x70 [dm_mod]\n [<ffffffffa000387a>] ? dm_request_fn+0x18a/0x290 [dm_mod]\n [<ffffffff81255662>] ? __generic_unplug_device+0x32/0x40\n [<ffffffff8125569e>] ? generic_unplug_device+0x2e/0x50\n [<ffffffffa0002788>] ? dm_unplug_all+0x68/0x70 [dm_mod]\n [<ffffffff812503b6>] ? blk_unplug_work+0x36/0x70\n [<ffffffff81250380>] ? blk_unplug_work+0x0/0x70\n [<ffffffff8108c760>] ? worker_thread+0x170/0x2a0\n [<ffffffff810920d0>] ? autoremove_wake_function+0x0/0x40\n [<ffffffff8108c5f0>] ? worker_thread+0x0/0x2a0\n [<ffffffff81091d66>] ? kthread+0x96/0xa0\n [<ffffffff8100c14a>] ? child_rip+0xa/0x20\n [<ffffffff81091cd0>] ? kthread+0x0/0xa0\n [<ffffffff8100c140>] ? child_rip+0x0/0x20\ndo_IRQ: 0.137 No irq handler for vector (irq -1)\n\u00ffirq 16: nobody cared (try booting with the \"irqpoll\" option)\n \nBut unable to collect vmcore file due to below error on console:\n\nDo you have a strange power saving mode enabled?\nDazed and confused, but trying to continue\nUhhuh. NMI received for unknown reason 35 on CPU 0. Will retry and update.\n\n--- Additional comment from Chad Dupuis on 2012-07-03 13:35:29 EDT ---\n\n>  \n> But unable to collect vmcore file due to below error on console:\n> \n> Do you have a strange power saving mode enabled?\n> Dazed and confused, but trying to continue\n> Uhhuh. NMI received for unknown reason 35 on CPU 0. Will retry and update.\n\nCrud.  Does this happen even if you trigger a test crash via /proc/sysrq-trigger?  If we can't get a crash dump I can create a patch to capture the information using counters and print it to the error log.\n\n--- Additional comment from Ayyanar on 2012-07-03 14:08:59 EDT ---\n\nHit panic again on FCoE QLogic host and copied vmcore.flat file here:\n\nftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2  \n\nback trace is as follows:\n\ncrash> bt -a\nPID: 24807  TASK: ffff88021764aae0  CPU: 0   COMMAND: \"dt.stable\"\n #0 [ffff880040207e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040207ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040207ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040207ef0] notify_die at ffffffff810980ae\n #4 [ffff880040207f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040207f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+50]\n    RIP: ffffffff81500012  RSP: ffff88021fb013d8  RFLAGS: 00000093\n    RAX: 0000000000008d5e  RBX: ffff880226974680  RCX: 0000000000008d5c\n    RDX: 0000000000000246  RSI: 0000000000011220  RDI: ffff880224f73840\n    RBP: ffff88021fb013d8   R8: ffff8801c1cb3e38   R9: 0000000000000000\n    R10: 0000000000000000  R11: 000000000000040c  R12: ffff880224f735e0\n    R13: ffff8802245a4000  R14: ffff880226d72080  R15: ffff880224f73800\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff88021fb013d8] _spin_lock_irqsave at ffffffff81500012\n #7 [ffff88021fb013e0] qla24xx_start_scsi at ffffffffa01f4af7 [qla2xxx]\n #8 [ffff88021fb01490] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #9 [ffff88021fb01620] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n#10 [ffff88021fb01680] scsi_dispatch_cmd at ffffffff81363411\n#11 [ffff88021fb016b0] scsi_request_fn at ffffffff8136af0d\n#12 [ffff88021fb01730] __blk_run_queue at ffffffff812554c1\n#13 [ffff88021fb01750] elv_insert at ffffffff8124f6f8\n#14 [ffff88021fb01790] __elv_add_request at ffffffff8124f7ea\n#15 [ffff88021fb017c0] blk_insert_cloned_request at ffffffff81254ead\n#16 [ffff88021fb017f0] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#17 [ffff88021fb01810] dm_request_fn at ffffffffa000387a [dm_mod]\n#18 [ffff88021fb01870] __generic_unplug_device at ffffffff81255662\n#19 [ffff88021fb01890] generic_unplug_device at ffffffff8125569e\n#20 [ffff88021fb018b0] dm_unplug_all at ffffffffa0002788 [dm_mod]\n#21 [ffff88021fb018e0] blk_unplug at ffffffff81250324\n#22 [ffff88021fb01900] dm_table_unplug_all at ffffffffa00041fc [dm_mod]\n#23 [ffff88021fb01970] dm_unplug_all at ffffffffa0002768 [dm_mod]\n#24 [ffff88021fb019a0] blk_unplug at ffffffff81250324\n#25 [ffff88021fb019c0] blk_backing_dev_unplug at ffffffff81250372\n#26 [ffff88021fb019d0] block_sync_page at ffffffff811ac57e\n#27 [ffff88021fb019e0] sync_page at ffffffff811140f8\n#28 [ffff88021fb019f0] __wait_on_bit at ffffffff814fe97f\n#29 [ffff88021fb01a40] wait_on_page_bit at ffffffff81114333\n#30 [ffff88021fb01aa0] grab_cache_page_write_begin at ffffffff81115250\n#31 [ffff88021fb01af0] ext4_da_write_begin at ffffffffa026d9f4 [ext4]\n#32 [ffff88021fb01b90] generic_file_buffered_write at ffffffff81114ab3\n#33 [ffff88021fb01c60] __generic_file_aio_write at ffffffff81116450\n#34 [ffff88021fb01d20] generic_file_aio_write at ffffffff811166ef\n#35 [ffff88021fb01d70] ext4_file_write at ffffffffa0262131 [ext4]\n#36 [ffff88021fb01dc0] do_sync_write at ffffffff8117ad6a\n#37 [ffff88021fb01ef0] vfs_write at ffffffff8117b068\n#38 [ffff88021fb01f30] sys_write at ffffffff8117ba81\n#39 [ffff88021fb01f80] sysenter_dispatch at ffffffff8104a820\n    RIP: 00000000f779e430  RSP: 00000000ff997d64  RFLAGS: 00000296\n    RAX: ffffffffffffffda  RBX: ffffffff8104a820  RCX: 00000000085ed000\n    RDX: 000000000000c800  RSI: 00000000085d7060  RDI: 0000000000000000\n    RBP: 00000000ff997da8   R8: 0000000000000000   R9: 0000000000000000\n    R10: 0000000000000000  R11: 0000000000000000  R12: 0000000000000000\n    R13: 0000000000000000  R14: 0000000000000003  R15: 0000000000000000\n    ORIG_RAX: 0000000000000004  CS: 0023  SS: 002b\n\nPID: 24810  TASK: ffff880220e76aa0  CPU: 1   COMMAND: \"dt.stable\"\n #0 [ffff8800402837d8] machine_kexec at ffffffff8103281b\n #1 [ffff880040283838] crash_kexec at ffffffff810ba662\n #2 [ffff880040283908] panic at ffffffff814fd121\n #3 [ffff880040283988] qla24xx_start_scsi at ffffffffa01f4f16 [qla2xxx]\n #4 [ffff880040283a38] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #5 [ffff880040283bc8] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n #6 [ffff880040283c28] scsi_dispatch_cmd at ffffffff81363411\n #7 [ffff880040283c58] scsi_request_fn at ffffffff8136af0d\n #8 [ffff880040283cd8] __blk_run_queue at ffffffff812554c1\n #9 [ffff880040283cf8] elv_insert at ffffffff8124f6f8\n#10 [ffff880040283d38] __elv_add_request at ffffffff8124f7ea\n#11 [ffff880040283d68] blk_insert_cloned_request at ffffffff81254ead\n#12 [ffff880040283d98] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#13 [ffff880040283db8] dm_request_fn at ffffffffa000387a [dm_mod]\n#14 [ffff880040283e18] __blk_run_queue at ffffffff812554c1\n#15 [ffff880040283e38] blk_run_queue at ffffffff81255610\n#16 [ffff880040283e58] rq_completed at ffffffffa0001dac [dm_mod]\n#17 [ffff880040283e78] dm_softirq_done at ffffffffa000257f [dm_mod]\n#18 [ffff880040283eb8] blk_done_softirq at ffffffff8125d605\n#19 [ffff880040283ee8] __do_softirq at ffffffff81073ec1\n#20 [ffff880040283f58] call_softirq at ffffffff8100c24c\n#21 [ffff880040283f70] do_softirq at ffffffff8100de85\n#22 [ffff880040283f90] irq_exit at ffffffff81073ca5\n#23 [ffff880040283fa0] smp_call_function_single_interrupt at ffffffff8102a905\n#24 [ffff880040283fb0] call_function_single_interrupt at ffffffff8100bdb3\n--- <IRQ stack> ---\n#25 [ffff88021fb07f58] call_function_single_interrupt at ffffffff8100bdb3\n    RIP: 00000000080585a2  RSP: 00000000ff8baf40  RFLAGS: 00000246\n    RAX: 0000000000000042  RBX: 00000000ff8baf58  RCX: 0000000000000000\n    RDX: 000000000000009d  RSI: 000000000848749d  RDI: 000000000000849d\n    RBP: ffffffff8100bdae   R8: 0000000000000000   R9: 0000000000000000\n    R10: 0000000000000000  R11: 0000000000000000  R12: 0000000000000000\n    R13: 0000000000000000  R14: 0000000000000000  R15: 0000000000000000\n    ORIG_RAX: ffffffffffffff04  CS: 0023  SS: 002b\n\nPID: 36     TASK: ffff88022a4d0040  CPU: 2   COMMAND: \"kblockd/2\"\n #0 [ffff880040307e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040307ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040307ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040307ef0] notify_die at ffffffff810980ae\n #4 [ffff880040307f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040307f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+47]\n    RIP: ffffffff8150000f  RSP: ffff88022a4d9900  RFLAGS: 00000093\n    RAX: 0000000000008d5f  RBX: ffff8801c1efb080  RCX: 0000000000008d5c\n    RDX: 0000000000000246  RSI: 0000000000011220  RDI: ffff880224f73840\n    RBP: ffff88022a4d9900   R8: ffff8801a5f49508   R9: 0000000000000000\n    R10: 0000000000000000  R11: 000000000000040d  R12: ffff880224f735e0\n    R13: ffff8802245a4000  R14: ffff880226d05380  R15: ffff880224f73800\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff88022a4d9900] _spin_lock_irqsave at ffffffff8150000f\n #7 [ffff88022a4d9908] qla24xx_start_scsi at ffffffffa01f4af7 [qla2xxx]\n #8 [ffff88022a4d99b8] qla24xx_dif_start_scsi at ffffffffa01f5876 [qla2xxx]\n #9 [ffff88022a4d9b48] qla2xxx_queuecommand at ffffffffa01dbdbc [qla2xxx]\n#10 [ffff88022a4d9ba8] scsi_dispatch_cmd at ffffffff81363411\n#11 [ffff88022a4d9bd8] scsi_request_fn at ffffffff8136af0d\n#12 [ffff88022a4d9c58] __blk_run_queue at ffffffff812554c1\n#13 [ffff88022a4d9c78] elv_insert at ffffffff8124f6f8\n#14 [ffff88022a4d9cb8] __elv_add_request at ffffffff8124f7ea\n#15 [ffff88022a4d9ce8] blk_insert_cloned_request at ffffffff81254ead\n#16 [ffff88022a4d9d18] dm_dispatch_request at ffffffffa00023ac [dm_mod]\n#17 [ffff88022a4d9d38] dm_request_fn at ffffffffa000387a [dm_mod]\n#18 [ffff88022a4d9d98] __generic_unplug_device at ffffffff81255662\n#19 [ffff88022a4d9db8] generic_unplug_device at ffffffff8125569e\n#20 [ffff88022a4d9dd8] dm_unplug_all at ffffffffa0002788 [dm_mod]\n#21 [ffff88022a4d9e08] blk_unplug_work at ffffffff812503b6\n#22 [ffff88022a4d9e38] worker_thread at ffffffff8108c760\n#23 [ffff88022a4d9ee8] kthread at ffffffff81091d66\n#24 [ffff88022a4d9f48] kernel_thread at ffffffff8100c14a\n\nPID: 0      TASK: ffff88022ae2eaa0  CPU: 3   COMMAND: \"swapper\"\n #0 [ffff880040387e90] crash_nmi_callback at ffffffff81029df6\n #1 [ffff880040387ea0] notifier_call_chain at ffffffff81503325\n #2 [ffff880040387ee0] atomic_notifier_call_chain at ffffffff8150338a\n #3 [ffff880040387ef0] notify_die at ffffffff810980ae\n #4 [ffff880040387f20] do_nmi at ffffffff81500fa3\n #5 [ffff880040387f50] nmi at ffffffff815008b0\n    [exception RIP: _spin_lock_irqsave+47]\n    RIP: ffffffff8150000f  RSP: ffff880040383e88  RFLAGS: 00000097\n    RAX: 0000000000008d5d  RBX: ffff880224f73800  RCX: 0000000000008d5c\n    RDX: 0000000000000086  RSI: ffff880225b3fec0  RDI: ffff880224f73840\n    RBP: ffff880040383e88   R8: 0000000000010000   R9: ffff88022ae37ee8\n    R10: 0000000000000000  R11: 0000000000000001  R12: ffff880224f73840\n    R13: 0000000000000000  R14: ffffc90000c3c000  R15: ffff880225b3fec0\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n #6 [ffff880040383e88] _spin_lock_irqsave at ffffffff8150000f\n #7 [ffff880040383e90] qla24xx_msix_rsp_q at ffffffffa01f863d [qla2xxx]\n #8 [ffff880040383ed0] handle_IRQ_event at ffffffff810db800\n #9 [ffff880040383f20] handle_edge_irq at ffffffff810ddf8e\n#10 [ffff880040383f60] handle_irq at ffffffff8100df09\n#11 [ffff880040383f80] do_IRQ at ffffffff81505aec\n--- <IRQ stack> ---\n#12 [ffff88022ae37e38] ret_from_intr at ffffffff8100ba53\n    [exception RIP: mwait_idle+119]\n    RIP: ffffffff81014877  RSP: ffff88022ae37ee8  RFLAGS: 00000246\n    RAX: 0000000000000000  RBX: ffff88022ae37ef8  RCX: 0000000000000000\n    RDX: 0000000000000000  RSI: ffff88022ae37fd8  RDI: ffff88022ac5d840\n    RBP: ffffffff8100ba4e   R8: 0000000000000000   R9: 0000000000000001\n    R10: 0000000000000000  R11: 0000000000000000  R12: ffffffff81bde6d0\n    R13: 0000000000000000  R14: ffffffff810f31d3  R15: ffff88022ae37e68\n    ORIG_RAX: ffffffffffffff76  CS: 0010  SS: 0018\n#13 [ffff88022ae37f00] cpu_idle at ffffffff81009e06\ncrash>\n\n--- Additional comment from Chad Dupuis on 2012-07-03 15:02:53 EDT ---\n\nThere doesn't appear to be a file at ftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2.\n\n--- Additional comment from Ayyanar on 2012-07-03 15:38:16 EDT ---\n\n\n(In reply to comment #20)\n> There doesn't appear to be a file at\n> ftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.829739/vmcore.flat.bz2.\n\nIt should work now. please check.\n\n--- Additional comment from Chad Dupuis on 2012-07-03 15:56:46 EDT ---\n\nWorks now, thanks.\n\n--- Additional comment from Ayyanar on 2012-07-09 07:18:35 EDT ---\n\nChad, any update on this? Do you need any other logs to triage?\n\n--- Additional comment from Chad Dupuis on 2012-07-12 16:30:22 EDT ---\n\n- Still panics if the oustanding commands array fills up\n- Increases the outstanding commands array to 2048.\n\n--- Additional comment from Chad Dupuis on 2012-07-12 16:33:36 EDT ---\n\n(In reply to comment #23)\n> Chad, any update on this? Do you need any other logs to triage?\n\nCan you try the patch in comment 24?  I looked at the vmcore and:\n\na) The oustanding commands array is for a particular request queue really is filling up.\nb) The are all SCSI commands.\n\nI had wanted the dump to make sure that there weren't a lot of command internal to the driver that were outstanding.  Anyways, as a data point, let's try increasing the capacity of how many commands we can have outstanding as that is the best course of action at this point.\n\n--- Additional comment from Chad Dupuis on 2012-07-12 18:14:09 EDT ---\n\n(In reply to comment #25)\n> \n> Can you try the patch in comment 24?  I looked at the vmcore and:\n\nActually can you hold off on that...if we hit the condition again, we need to collect a firmware dump so I'll need to change the code slightly.\n\n--- Additional comment from Ayyanar on 2012-07-13 09:14:25 EDT ---\n\nChad, Just a data point to add here ,we have already tried increasing the MAX_OUTSTANDING_COMMANDS to 16384 and we are still seeing \"Start scsi failed\" messages after 18 iterations of faults. (after 16 hrs of IO with faults)\n\nJul 12 15:42:02 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077215a80.\n\nI hope oustanding commands array is quickly filling up. Do you want us to try with MAX_OUTSTANDING_COMMANDS=2048 and collect firmware dumps and vmcore Or MAX_OUTSTANDING_COMMANDS=16384 ??\n\nTo collect firmware dump can I use below patch along with firmware dump collect scripts?\n\n--- qla_iocb.c.orig     2012-07-13 18:24:32.970553006 +0530\n+++ qla_iocb.c  2012-07-13 18:30:25.057512551 +0530\n@@ -363,7 +363,11 @@ qla2x00_start_scsi(srb_t *sp)\n                        break;\n        }\n        if (index == MAX_OUTSTANDING_COMMANDS)\n-               goto queuing_error;\n+               /* Try to take a firmware dump to ascertain the\n+                        * state of the firmware */\n+                       ha->isp_ops->fw_dump(vha, 0);\n+             panic(\"scsi(%ld): req->outstanding_cmds is full\\n\", vha->host_no);\n+              goto queuing_error;\n\n        /* Map the sg table so we have an accurate count of sg entries needed */\n        if (scsi_sg_count(cmd)) {\n\n--- Additional comment from Ayyanar on 2012-07-13 09:52:46 EDT ---\n\nOne more info to share: with MAX_OUTSTANDING_COMMANDS=16384 first 18hrs including host FC port block (disable and enable) testing there is no \"Start sci failed: messages; after 18 hrs of faults with IO, I am seeing \"Start scsi failed\" messages only with host FC port block (disable and enable) testing. \n\nIt happens with one particular port (in this host it is fc host2)goes offline. \n\nFrom the logs,\n\nJul 12 15:40:33 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 12 15:42:02 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077215a80.\nJul 12 15:43:05 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 12 23:44:49 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)]\nJul 12 23:46:19 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff8802831490c0.\nJul 12 23:47:21 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 06:29:57 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 06:31:50 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880283214580\nJul 13 06:32:30 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 10:41:07 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 10:43:14 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff88006fecdcc0.\nJul 13 10:43:39 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 16:28:43 ibmx3650-210-104 **NATE**: disabling switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nJul 13 16:29:10 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:1: Start scsi failed rval=258 for cmd=ffff880077d27c80\nJul 13 16:31:15 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\n\n--- Additional comment from Ayyanar on 2012-07-17 14:43:06 EDT ---\n\n> Actually can you hold off on that...if we hit the condition again, we need\n> to collect a firmware dump so I'll need to change the code slightly.\n\nChad, can you give this patch?\n\n--- Additional comment from Chad Dupuis on 2012-07-17 17:30:50 EDT ---\n\n(In reply to comment #29)\n> > Actually can you hold off on that...if we hit the condition again, we need\n> > to collect a firmware dump so I'll need to change the code slightly.\n> \n> Chad, can you give this patch?\n\nThe firmware dump is probably not needed at this point.  There is just a large number of commands that are submitted in a short time that we're not getting responses for to clear out the outstanding_cmds array.  It's possible that the fact that this occurs when a port goes down has something to do with the behavior.\n\n--- Additional comment from Ayyanar on 2012-07-18 09:28:18 EDT ---\n\n> The firmware dump is probably not needed at this point.  There is just a\n> large number of commands that are submitted in a short time that we're not\n> getting responses for to clear out the outstanding_cmds array.  It's\n> possible that the fact that this occurs when a port goes down has something\n> to do with the behavior.\nCan you create a patch to clean out the outstanding_cmds array? Or you want me to try with MAX_OUTSTANDING_COMMANDS=2048 and collect vmcore?\n\n--- Additional comment from Martin George on 2012-07-19 10:02:35 EDT ---\n\n(In reply to comment #30)\n> The firmware dump is probably not needed at this point.  There is just a\n> large number of commands that are submitted in a short time that we're not\n> getting responses for to clear out the outstanding_cmds array.  It's\n> possible that the fact that this occurs when a port goes down has something\n> to do with the behavior.\n\nPoint is with the current default of 1024 max outstanding commands in the ISP queue, we are hitting the problem consistently during straight IO itself i.e. even before fabric faults are introduced. But after bumping this up to a higher value like say 16384, the issue is harder to reproduce, but eventually do hit it when ports are down.\n\n--- Additional comment from Ayyanar on 2012-07-20 06:50:10 EDT ---\n\nChad,\n\nGiven that we are hitting IO outages during fabric faults in our tests on RHEL6.3 QLogic FC, do you think the above QLogic command queuing problem is contributing to this? Have you identified a fix for this yet?\n\n--- Additional comment from Chad Dupuis on 2012-07-20 16:02:26 EDT ---\n\n(In reply to comment #32)\n> \n> Point is with the current default of 1024 max outstanding commands in the\n> ISP queue, we are hitting the problem consistently during straight IO itself\n> i.e. even before fabric faults are introduced. But after bumping this up to\n> a higher value like say 16384, the issue is harder to reproduce, but\n> eventually do hit it when ports are down.\n\nThis is a good data point to have.  There is a time when a port goes down and when we tell the FC transport that to block the port (essentially to start the dev_loss_timer) where we fill up with outstanding commands but obviously we're not going to get any response to those commands from a dead port so that might explain why you still see this with an value of 16384.\n\n--- Additional comment from Chad Dupuis on 2012-07-20 16:04:19 EDT ---\n\n(In reply to comment #33)\n> Chad,\n> \n> Given that we are hitting IO outages during fabric faults in our tests on\n> RHEL6.3 QLogic FC, do you think the above QLogic command queuing problem is\n> contributing to this?\n\nIt's probably contributing since once we're full we can't accept anymore commands until we start receiving responses to the outstanding ones.\n\n> Have you identified a fix for this yet?\n\nNo, not yet.\n\n--- Additional comment from Ayyanar on 2012-07-25 04:06:39 EDT ---\n\nMarking the severity as 'Urgent' as this seems to contribute towards IO outages on the host.\n\n--- Additional comment from Chad Dupuis on 2012-07-31 10:33:36 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-07-31 10:40:10 EDT ---\n\nNetApp, could you try the patch attached in comment 37?\n\nIt, as you proposed before, increases the size of the oustanding_cmds array but only to 2048 at this point.  We don't want to increase the size of this particular array since this memory will be allocated for the entire lifetime of the driver for each adapter.  However, since in previous testing it was shown that increasing the size of the array did help and further analysis of the initial dump taken showed that there can be a bursty behavior in how commands are submitted to the driver it seems to make sense to increase the capacity here.\n\n--- Additional comment from Ayyanar on 2012-08-06 10:06:11 EDT ---\n\n\n\n--- Additional comment from Ayyanar on 2012-08-06 10:10:17 EDT ---\n\nChad,\n\nRan test with patched kernel in comment #37 and message file attached.\n\nI am still seeing \u201cStart scsi failed\u201d messages only during FC port block test.\n\nAug  3 04:29:24 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.1]-3813:6: Start scsi failed rval=258 for cmd=ffff88028365e9c0.\nAug  3 11:59:24 ibmx3650-210-104 kernel: qla2xxx [0000:1a:00.0]-3813:5: Start scsi failed rval=258 for cmd=ffff880077b689c0.\n\nAug  3 04:28:24 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\nAug  3 11:59:44 ibmx3650-210-104 **NATE**: enabled switch port (29) on switch (brcd5300-210-38) for device (ibmx3650-210-104)\n\n--- Additional comment from Chad Dupuis on 2012-08-07 10:05:20 EDT ---\n\nThe results at least are better.  In this latest test, when you see the start_scsi errors, does I/O eventually recover?\n\n--- Additional comment from Ayyanar on 2012-08-07 10:29:35 EDT ---\n\n(In reply to comment #41)\n> The results at least are better.  In this latest test, when you see the\n> start_scsi errors, does I/O eventually recover?\n\nYes IO recover but I am seeing IO outages and path validation failures.\n\nPath failures (path state remain as \"failed ready running\" after 10mins of fault recovery) are seen only on QLogic host.\n\n--- Additional comment from Martin George on 2012-08-09 11:52:10 EDT ---\n\nThe behavior is definitely better with the patch from comment #37, but it does not seem to provide a complete fix for the issue. And as described by Ayyanar above, intermittent IO outages & path failures are still seen after applying this patch.\n\nSo how do we proceed here?\n\n--- Additional comment from Chad Dupuis on 2012-08-10 10:53:49 EDT ---\n\nMartin, I believe the other part of the equation here is to throttle down the queue depth of the attached devices when we essentially hit a host queue full condition.  lpfc and libfc have algorithms (of varying complexity) to ramp down the queue depth of attached devices when they run into a condition where the driver runs out of buffers to send commands.  This is the direction I believe we should head here.\n\n--- Additional comment from Martin George on 2012-08-21 04:13:03 EDT ---\n\nChad,\n\nDo you have any updates on this?\n\n--- Additional comment from Chad Dupuis on 2012-08-22 09:24:00 EDT ---\n\nHi Martin,\n\nStill working on this.  Should hopefully have a patch for you to test by the end of the week or early next week.\n\n--- Additional comment from Chad Dupuis on 2012-08-28 13:53:57 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-08-28 13:59:24 EDT ---\n\nNetapp, please try the patch from comment 47.  It still contains the changes that were in comment 37 but also adds:\n\n- A back-off algorithm when we run out of host resources.  This is done by throttling the queue depth of all devices attached to the adapter geometically (i.e. dividing by two), waiting for a settle period (60 seconds) and then slowly increasing the queue depth for each device on the adapter by 1 every 30 seconds until we hit ql2xmaxqdepth.\n\n- I've removed the \"fc_remote_port_chkready failed\" and \"Start scsi failed\" debug messages which are clogging up the system logs.\n\n--- Additional comment from Chad Dupuis on 2012-09-11 10:04:02 EDT ---\n\nAny update on this; does this help in your testing?\n\n--- Additional comment from Ayyanar on 2012-09-11 12:49:18 EDT ---\n\n\n\n--- Additional comment from Ayyanar on 2012-09-11 12:55:28 EDT ---\n\n>Any update on this; does this help in your testing?\n\nSince \"Start scsi failed\" messages are not logged, I am not sure we are hitting those conditions or not.\n\nIO delay (no IO progress during FC port block) is seen with this patch too.\nWill update the status once I am done with few more testing.\n\n--- Additional comment from Chad Dupuis on 2012-09-11 13:11:40 EDT ---\n\n(In reply to comment #51)\n> >Any update on this; does this help in your testing?\n> \n> Since \"Start scsi failed\" messages are not logged, I am not sure we are\n> hitting those conditions or not.\n> \n> IO delay (no IO progress during FC port block) is seen with this patch too.\n> Will update the status once I am done with few more testing.\n\nThank you for the update Ayyanar.  For the most part you are not hitting the host exhaustion but it does occur a few times.  I saw the following statements:\n\nSep  2 20:13:50 ibmx3550-229-31 kernel: qla2xxx [0000:1a:00.0]-ffff:4: 4:0:0: Ramping down queue depth to 16\n\nwhich does mean we are hitting the condition occassionally.  The idea about ramping down the queue depth was to allow the host some time to recover before resuming full throttle i/o.  Are your i/o outages any shorter?  If they are not then the whole ramp up/ramp down thing may not be worth the effort.\n\n--- Additional comment from Ayyanar on 2012-09-12 07:01:12 EDT ---\n\n Are your i/o outages any shorter?  If they are not then the whole ramp up/ramp down thing may not be worth the effort.\n\nYes. IO outages are shorter now (less than 180 sec). With the last run I hit mailbox command timed out issue, and firmware dump is cleared.\n\nkernel: qla2xxx [0000:1a:00.1]-d001:2: Firmware dump saved to temp buffer (2/ffffc9001286a000).\nkernel: qla2xxx [0000:1a:00.1]-101e:2: Mailbox cmd timeout occured, cmd=0x54, mb[0]=0x54. Scheduling ISP abort\nkernel: qla2xxx [0000:1a:00.1]-00af:2: Performing ISP error recovery - ha=ffff880037464000.\nkernel: qla2xxx [0000:1a:00.1]-4801:2: DPC handler waking up.\nkernel: qla2xxx [0000:1a:00.1]-4802:2: dpc_flags=0x8.\nkernel: qla2xxx [0000:1a:00.1]-4800:2: DPC handler sleeping.\nkernel: qla2xxx [0000:1a:00.1]-705e:2: Raw firmware dump ready for read on (2).\nkernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).\n\n--- Additional comment from Ayyanar on 2012-09-12 07:02:14 EDT ---\n\n\n\n--- Additional comment from Chad Dupuis on 2012-09-14 09:21:01 EDT ---\n\n(In reply to comment #53)\n>  Are your i/o outages any shorter?  If they are not then the whole ramp\n> up/ramp down thing may not be worth the effort.\n> \n> Yes. IO outages are shorter now (less than 180 sec). With the last run I hit\n> mailbox command timed out issue, and firmware dump is cleared.\n> \n> kernel: qla2xxx [0000:1a:00.1]-d001:2: Firmware dump saved to temp buffer\n> (2/ffffc9001286a000).\n> kernel: qla2xxx [0000:1a:00.1]-101e:2: Mailbox cmd timeout occured,\n> cmd=0x54, mb[0]=0x54. Scheduling ISP abort\n> kernel: qla2xxx [0000:1a:00.1]-00af:2: Performing ISP error recovery -\n> ha=ffff880037464000.\n> kernel: qla2xxx [0000:1a:00.1]-4801:2: DPC handler waking up.\n> kernel: qla2xxx [0000:1a:00.1]-4802:2: dpc_flags=0x8.\n> kernel: qla2xxx [0000:1a:00.1]-4800:2: DPC handler sleeping.\n> kernel: qla2xxx [0000:1a:00.1]-705e:2: Raw firmware dump ready for read on\n> (2).\n> kernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).\n\nDo you have the firmware dump?  Would be good for analysis.  Looks like it occurred after a switch port disable during the latter half of a fabric rescan.\n\n--- Additional comment from Ayyanar on 2012-09-21 06:42:27 EDT ---\n\nDo you have the firmware dump?\n\nNo Chad. It is cleared.\n\nkernel: qla2xxx [0000:1a:00.1]-705d:2: Firmware dump cleared on (2).\n\nexcept, this one instance I have not seen firmware dump.\n\n--- Additional comment from Rajashekhar M A on 2012-09-21 08:48:37 EDT ---\n\nI tried the patch in comment 47 and it seems to have worked. I am not seeing the messages and also paths come up fine within expected time.\n\nCould you please push this fix to the upcoming errata? This fixes a couple of issues that we are hitting.\n\n--- Additional comment from Martin George on 2012-09-24 07:41:11 EDT ---\n\nSo looks like the patch from comment #47 has indeed helped improve the behavior here on QLogic FC hosts.\n\nRequesting a z-stream errata fix for the same since this involves path failures and long IO outages on the host.\n\n--- Additional comment from Chad Dupuis on 2012-10-18 10:03:08 EDT ---\n\nLatest version of the patch(es) to handle host resource exhaustion.\n\n--- Additional comment from Ayyanar on 2012-11-01 04:56:07 EDT ---\n\nRan test with your latest patch from comment#59 and still seeing IO outages and attached log.\n\n--- Additional comment from Ayyanar on 2012-11-01 04:59:40 EDT ---\n\n\n\n--- Additional comment from Ayyanar on 2012-11-02 06:21:01 EDT ---\n\n\"Start scsi failed messages\" are not seen from logs. But Queue depth adjusting messages and Abort messages are there.\n\nkernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for nexus=1:0:16.\nkernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for nexus=1:0:17.\n\nkernel: qla2xxx [0000:0c:00.1]-8802:1: Aborting from RISC nexus=1:0:1 sp=ffff881047f36680 cmd=ffff88104671c3c0\nkernel: qla2xxx [0000:0c:00.1]-3822:1: FCP command status: 0x5-0x0 (0x80000) nexus=1:0:1 portid=015000 oxid=0x4a2 cdb=2a00007c2a0000008000 len=0x10000 rsp_info=0x0 resid=0x0 fw_resid=0x0.\nkernel: qla2xxx [0000:0c:00.1]-8804:1: Abort command mbx success cmd=ffff88104671c3c0.\n\n--- Additional comment from Chad Dupuis on 2012-11-02 09:21:22 EDT ---\n\n(In reply to comment #62)\n> \"Start scsi failed messages\" are not seen from logs. But Queue depth\n> adjusting messages and Abort messages are there.\n> \n> kernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for\n> nexus=1:0:16.\n> kernel: qla2xxx [0000:0c:00.1]-3829:1: Queue depth adjusted-down to 31 for\n> nexus=1:0:17.\n> \n> kernel: qla2xxx [0000:0c:00.1]-8802:1: Aborting from RISC nexus=1:0:1\n> sp=ffff881047f36680 cmd=ffff88104671c3c0\n> kernel: qla2xxx [0000:0c:00.1]-3822:1: FCP command status: 0x5-0x0 (0x80000)\n> nexus=1:0:1 portid=015000 oxid=0x4a2 cdb=2a00007c2a0000008000 len=0x10000\n> rsp_info=0x0 resid=0x0 fw_resid=0x0.\n> kernel: qla2xxx [0000:0c:00.1]-8804:1: Abort command mbx success\n> cmd=ffff88104671c3c0.\n\nThe \"Queue depth adjusted-down\" messages from this log are actually the result of queue fulls being reported by the target:\n\nNov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE FULL detected. \n\nI did not see any messages that pointed to resource exhaustion on the driver side.\n\n--- Additional comment from Martin George on 2012-11-06 06:11:12 EST ---\n\n(In reply to comment #63)\n> The \"Queue depth adjusted-down\" messages from this log are actually the\n> result of queue fulls being reported by the target:\n> \n> Nov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE\n> FULL detected. \n> \n> I did not see any messages that pointed to resource exhaustion on the driver\n> side.\n\nJust curious. When hitting a QUEUE FULL scenario, the target expects the host to back off and wait for at least one command to return before resuming IO. Is that expectation met with this patchset?\n\n--- Additional comment from Chad Dupuis on 2012-11-06 09:42:23 EST ---\n\n(In reply to comment #64)\n> (In reply to comment #63)\n> > The \"Queue depth adjusted-down\" messages from this log are actually the\n> > result of queue fulls being reported by the target:\n> > \n> > Nov  1 02:52:34 ibmx3550-210-99 kernel: qla2xxx [0000:0c:00.0]-3820:0: QUEUE\n> > FULL detected. \n> > \n> > I did not see any messages that pointed to resource exhaustion on the driver\n> > side.\n> \n> Just curious. When hitting a QUEUE FULL scenario, the target expects the\n> host to back off and wait for at least one command to return before resuming\n> IO. Is that expectation met with this patchset?\n\nThe target QUEUE FULL scenario is actually controlled by the SCSI layer in RHEL 6.  The qla2xxx driver simply returns the queue full status back to the SCSI layer and it is there that the queue depth is ramped down.  From what I can see in the SCSI layer, the host will ramp down it's queue depth but will not completely block.\n\n--- Additional comment from Martin George on 2012-11-19 14:00:52 EST ---\n\nChad,\n\nIgnore comment #60 - the IO outages there seem to be caused by a target bug (that causes frames to be dropped), and nothing to do with your patchset.\n\nAnd we've now run extensive tests with your latest patchset provided in comment #59. And the results look good - no longer seeing the above resource exhaustion issues on the QLogic host.\n\nSo requesting you to include this patchset now in the inbox qla2xxx driver.\n\n--- Additional comment from Martin George on 2012-11-19 22:57:43 EST ---\n\nChris,\n\nWithout this fix, the customer would end up seeing path failures & occasional long IO outages during fabric faults. So requesting for a z-stream 6.3.z errata fix for the same.\n\nThanks,\n-Martin\n\n--- Additional comment from Chris Tatman on 2012-11-19 23:54:41 EST ---\n\nThanks Martin,\n\nThe zstream request process has been initiated and the flags are all set.\n\n--Chris\n\n--- Additional comment from Chris Tatman on 2012-11-28 14:21:54 EST ---\n\nHi Chad,\n\nI see that the fix has been posted for review.  Has it actually been accepted into 6.4 yet?  Martin has stated their testing with 6.4 alpha builds is still failing.\n\nJust trying to figure out when/where we can expect a package that we can test.\n\nThanks!\n\n--Chrisa\n\n--- Additional comment from Chad Dupuis on 2012-11-28 16:43:40 EST ---\n\n(In reply to comment #69)\n> Hi Chad,\n> \n> I see that the fix has been posted for review.  Has it actually been\n> accepted into 6.4 yet?\n\nNot yet, it's going through the review process.\n\n>  Martin has stated their testing with 6.4 alpha\n> builds is still failing.\n> \n> Just trying to figure out when/where we can expect a package that we can\n> test.\n> \n> Thanks!\n> \n> --Chrisa\n\n--- Additional comment from Rob Evers on 2012-12-05 16:55:36 EST ---\n\nUnless I'm mistaken, this still isn't posted upstream and that is blocking acceptance into rhel6.4.\n\n--- Additional comment from Chris Tatman on 2012-12-06 08:57:14 EST ---\n\nRob,\n\nWho is responsible for posting this upstream?\n\n--Chris\n\n--- Additional comment from Chris Tatman on 2012-12-06 09:08:08 EST ---\n\nJust saw this email response from Chad this morning:\n\n\"The issue has been with upstream posting.  We plan on submitting this to linux-scsi tomorrow.\"\n\nReminder:\nWe have missed the kernel 7 deadline and we have only one zstream kernel left to us for 6.3.  \n\nThe deadline for patch acceptance for Kernel 8 for 6.3 is: 1/09/2012, so we have a little time left to us.  But this patch needs to be accepted upstream before we can complete the post review on our end and get this into 6.4 and 6.3.z.\n\nThanks!\n\n--Chris\n\n--- Additional comment from Chris Tatman on 2012-12-06 09:09:23 EST ---\n\nChad,\n\nSetting this bug to waiting on you while we are waiting for the upstream ack on the patch.\n\n--Chris\n\n--- Additional comment from Chad Dupuis on 2012-12-07 08:41:01 EST ---\n\nUpstream postings:\n\nqla2xxx: Determine the number of outstanding commands based on available resources.\nhttp://marc.info/?l=linux-scsi&m=135486817323393&w=2\n\nqla2xxx: Ramp down queue depth for attached SCSI devices when driver resources are low.\nhttp://marc.info/?l=linux-scsi&m=135486658422862&w=2\n\n--- Additional comment from Chad Dupuis on 2012-12-19 16:50:21 EST ---\n\nFYI, these two patches have been accepted upstream by the SCSI maintainer.\n\n--- Additional comment from Jarod Wilson on 2012-12-20 16:22:28 EST ---\n\nPatch(es)\n\n--- Additional comment from Barry Marson on 2013-01-07 16:44:52 EST ---\n\nWhile working another issue with gfs engineering, I ran a kernel with the driver/firmware changes associated with this bz and tested a grid based SAS (ISV) application on a 4 node cluster.  Card information is:\n\ndriver_version: 8.04.00.08.06.4-k\nfw_version:     5.08.00 (9496)\nmodel_desc:     PCI-Express Dual Port 4Gb Fibre Channel HBA\nmodel_name:     HPAE312A\n\n# lspci | grep -i logic\n0d:00.0 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)\n0d:00.1 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)\n\nThere was no performance regression with my tests, but I'm concerned with the volume of messages regarding 'Ramping down queue depths'.  My test configuration has 192 multipaths of which half are typically active.  During my 2.5 hour test run which generates around 2TB of I/O (45%reads 55% writes), each node logged around 10K messages.\n\nBarry\n\n--- Additional comment from Rob Evers on 2013-01-08 10:38:16 EST ---\n\nThe print output in question is:\n\nJan  7 12:08:23 pats kernel: qla2xxx [0000:0d:00.1]-3031:2: 2:7:12: Ramping down queue depth to 3\n\nBarry,\n\nCan you open another bug due to the prints here?\n\nThanks, Rob\n\n--- Additional comment from Rob Evers on 2013-01-09 12:47:22 EST ---\n\nOther performance data indicated a significant regression with this update.  The patches for this bugzilla are going to be reverted for rhel6.4.\n\n--- Additional comment from Gris Ge on 2013-01-15 23:17:16 EST ---\n\n(In reply to comment #88)\n> Other performance data indicated a significant regression with this update. \n> The patches for this bugzilla are going to be reverted for rhel6.4.\n\nRob,\nSince this bug is ON_QA, does it mean patch reverted or still on going?\n\n--- Additional comment from Rob Evers on 2013-01-16 11:09:27 EST ---\n\n(In reply to comment #89)\n> (In reply to comment #88)\n> > Other performance data indicated a significant regression with this update. \n> > The patches for this bugzilla are going to be reverted for rhel6.4.\n> \n> Rob,\n> Since this bug is ON_QA, does it mean patch reverted or still on going?\n\nWe are still waiting for some more performance data, and the revert is actually\non hold.  I suggest continuing as if this were being included for now.\n\n--- Additional comment from Gris Ge on 2013-02-07 21:41:42 EST ---\n\nWe havn't install the 8G FC qla25xx card into your test environment.\n\nRegression pass for 16G FC qla26xx card on kernel -358.\n\nhttps://beaker.engineering.redhat.com/jobs/375458\n\nSanityOnly.\n\n--- Additional comment from errata-xmlrpc on 2013-02-21 01:21:09 EST ---\n\nSince the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttp://rhn.redhat.com/errata/RHSA-2013-0496.html",
          "author": "salmy@redhat.com",
          "creation_time": "20131121T15:37:01",
          "bug_id": "1033136",
          "creator_id": "327275",
          "time": "20131121T15:37:01",
          "id": "6544510",
          "is_private": "False"
        },
        {
          "count": "1",
          "creator": "salmy@redhat.com",
          "text": "Creating this new BZ, since the original fix for BZ 829739 has been reverted from 6.6 (BZ 995576) and from 6.5.z (BZ 1032167).\n\nWe will pursue a fix to the original issue now represented by this BZ.",
          "author": "salmy@redhat.com",
          "creation_time": "20131121T15:42:44",
          "bug_id": "1033136",
          "creator_id": "327275",
          "time": "20131121T15:42:44",
          "id": "6544547",
          "is_private": "False"
        },
        {
          "count": "2",
          "attachment_id": "829496",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 829496\nqla2xxx: Set host can_queue value based on available resources.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20131126T21:21:26",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131126T21:21:26",
          "id": "6561050",
          "is_private": "False"
        },
        {
          "count": "3",
          "attachment_id": "829498",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 829498\nqla2xxx: Set host can_queue value based on available resources.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20131126T21:25:14",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131126T21:25:14",
          "id": "6561056",
          "is_private": "False"
        },
        {
          "count": "4",
          "attachment_id": "829499",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 829499\nqla2xxx: Reduce the time we wait for a command to complete during SCSI error handling.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20131126T21:26:24",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131126T21:26:24",
          "id": "6561058",
          "is_private": "False"
        },
        {
          "count": "5",
          "creator": "cdupuis@redhat.com",
          "text": "NetApp, can you you please test with the patches from Comment 3 and Comment 4 as well the revert patch in Bug 995576: https://bugzilla.redhat.com/attachment.cgi?id=798323?",
          "author": "cdupuis@redhat.com",
          "creation_time": "20131126T21:30:53",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131126T21:30:53",
          "id": "6561084",
          "is_private": "False"
        },
        {
          "count": "6",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to Chad Dupuis from comment #5)\n> NetApp, can you you please test with the patches from Comment 3 and Comment\n> 4 as well the revert patch in Bug 995576:\n> https://bugzilla.redhat.com/attachment.cgi?id=798323?\n\nI have tried with the above 3 patches on top of RHEL6U5 GA kernel and hit a mailbox cmd timeout issue.\n\nmodel:             QLE2562\ndriver version:    v.8.05.00.03.06.5-k2-debug\nfirmware version:  v.7.00.01 (90d5)\nkernel version:  2.6.32-431.el6\n\nNov 27 13:48:21 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-8802:5: Aborting from RISC nexus=5:3:20 sp=ffff880464771300 cmd=ffff88042870a280\nNov 27 13:48:28 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-d001:4: Firmware dump saved to temp buffer (4/ffffc900126ae000).\nNov 27 13:48:28 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-101e:4: Mailbox cmd timeout occurred, cmd=0x54, mb[0]=0x54. Scheduling ISP abort\nNov 27 13:48:28 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-00af:4: Performing ISP error recovery - ha=ffff880479b2e000.\nNov 27 13:48:28 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-d001:5: Firmware dump saved to temp buffer (5/ffffc90012854000).\nNov 27 13:48:28 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-101e:5: Mailbox cmd timeout occurred, cmd=0x54, mb[0]=0x54. Scheduling ISP abort",
          "author": "ayyanar@netapp.com",
          "creation_time": "20131128T17:15:43",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131128T17:15:43",
          "id": "6569353",
          "is_private": "False"
        },
        {
          "count": "7",
          "attachment_id": "830332",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 830332\nmessage file with all 3 patchs from comment #5",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20131128T17:20:30",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131128T17:20:30",
          "id": "6569363",
          "is_private": "False"
        },
        {
          "count": "8",
          "attachment_id": "830333",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 830333\nfw dump file with all 3 patches from comment #5",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20131128T17:23:52",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131128T17:23:52",
          "id": "6569372",
          "is_private": "False"
        },
        {
          "count": "9",
          "attachment_id": "830334",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 830334\nfw dump file with all 3 patches from comment #5",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20131128T17:26:05",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131128T17:26:05",
          "id": "6569376",
          "is_private": "False"
        },
        {
          "count": "10",
          "creator": "cdupuis@redhat.com",
          "text": "From you're configuration notes above it looks like you're running with the inbox firmware on 6.5.  I believe these issues have been addressed in the test firmware attached to https://bugzilla.redhat.com/show_bug.cgi?id=1015895#c24.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20131202T13:53:43",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131202T13:53:43",
          "id": "6575515",
          "is_private": "False"
        },
        {
          "count": "11",
          "creator": "ayyanar@netapp.com",
          "text": "Tried with the test firmware from https://bugzilla.redhat.com/show_bug.cgi?id=1015895#c24 + all 3 patches in comment #5 on top of RHEL6U5 GA kernel and still seeing firmware dumps due to mailbox command timeout.\n\nDec  2 16:09:33 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-d001:6: Firmware dump saved to temp buffer (6/ffffc90012854000).\nDec  2 16:09:33 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-1020:6: **** Failed mbx[0]=54, mb[1]=0, mb[2]=76d8, mb[3]=f100, cmd=54 ****.\n\ndriver version:    v.8.05.00.03.06.5-k2-debug\nfirmware version:  v.101.00.02 (b0d5)",
          "author": "ayyanar@netapp.com",
          "creation_time": "20131203T06:41:18",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131203T06:41:18",
          "id": "6579031",
          "is_private": "False"
        },
        {
          "count": "12",
          "attachment_id": "831913",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 831913\nfw_dump and messages file with the test fw + 3 patches",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20131203T06:45:25",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131203T06:45:25",
          "id": "6579040",
          "is_private": "False"
        },
        {
          "count": "13",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to Ayyanar from comment #11)\n> \n> driver version:    v.8.05.00.03.06.5-k2-debug\n> firmware version:  v.101.00.02 (b0d5)\n\nSomehow it looks like another test version was used for this test.  I was expecting version 102.00.02:\n\n[77415.919518] qla2xxx [0000:10:00.0]-00fc:23: ISP2532: PCIe (5.0GT/s x4) @ 0000:10:00.0 hdma+ host#=23 fw=102.00.02 (b0d5).\n\nIf I recall, the test version you used had some issues which were evidenced in the messages file from the test.  I'll attach the test firmware.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20131203T14:06:34",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131203T14:06:34",
          "id": "6580484",
          "is_private": "False"
        },
        {
          "count": "14",
          "attachment_id": "832110",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 832110\n8g Firmware version 102.00.02",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20131203T14:08:19",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131203T14:08:19",
          "id": "6580488",
          "is_private": "False"
        },
        {
          "count": "15",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to Chad Dupuis from comment #14)\n> Created attachment 832110 [details]\n> 8g Firmware version 102.00.02\n\nRan test with above firmware and seeing ADAPTER RESET.\n\nDec  4 10:05:11 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-8014:8: Wait for pending commands failed.\nDec  4 10:05:11 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-802b:8: BUS RESET FAILED nexus=8:1:9.\nDec  4 10:05:11 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-8018:8: ADAPTER RESET ISSUED nexus=8:1:9.\nDec  4 10:05:11 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-8819:8: qla2x00_wait_for_reset_ready return status=0.\nDec  4 10:05:11 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.1]-00af:8: Performing ISP error recovery - ha=ffff880277a2b000.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20131204T17:18:55",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131204T17:18:55",
          "id": "6585523",
          "is_private": "False"
        },
        {
          "count": "16",
          "attachment_id": "832765",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 832765\nmessages file with the firmware from comment#15",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20131204T17:22:30",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131204T17:22:30",
          "id": "6585541",
          "is_private": "False"
        },
        {
          "count": "17",
          "creator": "ayyanar@netapp.com",
          "text": "Chad, Any idea why Adapter resets were happened and how do we proceed from here?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20131211T14:11:29",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20131211T14:11:29",
          "id": "6606233",
          "is_private": "False"
        },
        {
          "count": "18",
          "creator": "cdupuis@redhat.com",
          "text": "The adapter resets are seen because we are trying to abort a request that has already been returned to the driver by the firmware.  I'm looking at a workaround.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20131218T21:03:38",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131218T21:03:38",
          "id": "6634353",
          "is_private": "False"
        },
        {
          "count": "19",
          "attachment_id": "839632",
          "author": "cdupuis@redhat.com",
          "text": "Created attachment 839632\nqla2xxx: Avoid escalating the SCSI error handler if the command is not found in firmware.\n\nFrom the last messages file provided, I believe we escalate to adapter reset as the aborts fail since the command is not in the firmware any more.  Thus we return failed for the abort operation which causes the SCSI error handler to escalate to the various reset where the task management command succeeds but then we fail waiting for DMA completion on a command that has already been returned.  This continues until we escalate to adapter reset which succeeds.\n\nI've attached a patch which prints out the reason for the abort failure as well as to change the return to SUCCESS if the command cannot be found to trigger the error handler to try a test unit ready to verify if the target is alive.  From the log files the target appears to have come back up from the drivers perspective so this should avoid the escalation of the SCSI error handler.",
          "creator": "cdupuis@redhat.com",
          "creation_time": "20131220T15:52:25",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20131220T15:52:25",
          "id": "6642161",
          "is_private": "False"
        },
        {
          "count": "20",
          "creator": "ayyanar@netapp.com",
          "text": "Ran test with the patch from comment #19 , on top of test firmware + 3 patches (comment #3) and still seeing fc_report_port becomes ''Blocked'' state.\n\nDec 24 06:35:30 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-8802:5: Aborting from RISC nexus=5:5:37 sp=ffff88040186cd40 cmd=ffff8804619aebc0\nDec 24 06:35:30 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-1090:5: Failed to complete IOCB -- completion status (31).\nDec 24 06:35:30 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-8803:5: Abort command mbx failed cmd=ffff8804619aebc0.\nDec 24 06:35:30 ibmx3550-229-144 kernel: qla2xxx [0000:1a:00.0]-801c:5: Abort command issued nexus=5:5:37 --  0 2002.\n\n[root@ibmx3550-229-144 ~]# cat /sys/class/fc_remote_ports/rport-*/port_state\nOnline\nOnline\nOnline\nOnline\nBlocked\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\nOnline\n\n\n3600a09803246696a5a5d44372d513655 dm-5 NETAPP,LUN C-Mode\nsize=10G features='4 queue_if_no_path pg_init_retries 50 retain_attached_hw_handle' hwhandler='1 alua' wp=rw\n|-+- policy='round-robin 0' prio=50 status=active\n| |- 5:0:4:4  sdkp  66:464  failed faulty running\n| |- 6:0:7:4  sdaca 135:544 active ready  running\n| |- 6:0:4:4  sdta  8:640   active ready  running\n| |- 6:0:3:4  sdpf  130:336 active ready  running\n| |- 5:0:3:4  sdgs  132:128 active ready  running\n| |- 6:0:0:4  sddk  71:32   active ready  running\n| |- 5:0:7:4  sdvx  69:560  active ready  running\n| `- 5:0:0:4  sde   8:64    active ready  running\n`-+- policy='round-robin 0' prio=10 status=enabled\n  |- 6:0:6:4  sdaac 132:512 active ready  running\n  |- 6:0:5:4  sdwv  70:688  active ready  running\n  |- 5:0:6:4  sdsa  134:480 active ready  running\n  |- 5:0:5:4  sdod  128:400 active ready  running\n  |- 6:0:2:4  sdlc  67:416  active ready  running\n  |- 6:0:1:4  sdhg  133:96  active ready  running\n  |- 5:0:2:4  sdda  70:128  active ready  running\n  `- 5:0:1:4  sdbc  67:96   active ready  running",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140102T08:10:12",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140102T08:10:12",
          "id": "6666047",
          "is_private": "False"
        },
        {
          "count": "21",
          "attachment_id": "844394",
          "author": "ayyanar@netapp.com",
          "text": "Created attachment 844394\nmessages file for comment #20",
          "creator": "ayyanar@netapp.com",
          "creation_time": "20140102T08:15:56",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140102T08:15:56",
          "id": "6666052",
          "is_private": "False"
        },
        {
          "count": "22",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to Ayyanar from comment #20)\n> \n> [root@ibmx3550-229-144 ~]# cat /sys/class/fc_remote_ports/rport-*/port_state\n> Online\n> Online\n> Online\n> Online\n> Blocked\n\nWhat is the 24 bit fabric ID of this blocked port?\n\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n> Online\n>",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140106T21:22:10",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140106T21:22:10",
          "id": "6678360",
          "is_private": "False"
        },
        {
          "count": "23",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to Chad Dupuis from comment #22)\n\n> What is the 24 bit fabric ID of this blocked port?\n> \nIt is 0x011003\n\n[root@ibmx3550-229-144 ~]# cat /sys/class/fc_remote_ports/rport-*/port_id\n0x011002\n0x011202\n0x011201\n0x011001\n0x011003\n0x011204\n0x011203\n0x011004\n0x011101\n0x011302\n0x011301\n0x011104\n0x011103\n0x011304\n0x011303\n0x011102",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140107T14:17:19",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140107T14:17:19",
          "id": "6681386",
          "is_private": "False"
        },
        {
          "count": "24",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nNetapp was asking about this one today.  Do you need any more data from them to continue?\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20140116T21:56:21",
          "bug_id": "1033136",
          "creator_id": "161143",
          "time": "20140116T21:56:21",
          "id": "6719190",
          "is_private": "True"
        },
        {
          "count": "25",
          "creator": "ayyanar@netapp.com",
          "text": "I continued the tests on this host with the patch from comment #19 , on top of test firmware + 3 patches (comment #3) and I hit a kernel crash after running IO for 2 days along with storage faults.\n\ndriver version:    v.8.05.00.03.06.5-k2-debug\nfirmware version:  v.102.00.02 (b0d5)\nmodel description: QLogic QLE2562\n\nbacktrace is as follows:\n\ncrash> bt\nPID: 0      TASK: ffff88027cdf8040  CPU: 5   COMMAND: ''swapper''\n #0 [ffff8800402a7b50] machine_kexec at ffffffff81038f3b\n #1 [ffff8800402a7bb0] crash_kexec at ffffffff810c5d92\n #2 [ffff8800402a7c80] panic at ffffffff81527201\n #3 [ffff8800402a7d00] watchdog_overflow_callback at ffffffff810e697d\n #4 [ffff8800402a7d20] __perf_event_overflow at ffffffff8111c857\n #5 [ffff8800402a7da0] perf_event_overflow at ffffffff8111ce24\n #6 [ffff8800402a7db0] intel_pmu_handle_irq at ffffffff81022d87\n #7 [ffff8800402a7e90] perf_event_nmi_handler at ffffffff8152ba59\n #8 [ffff8800402a7ea0] notifier_call_chain at ffffffff8152d515\n #9 [ffff8800402a7ee0] atomic_notifier_call_chain at ffffffff8152d57a\n#10 [ffff8800402a7ef0] notify_die at ffffffff810a154e\n#11 [ffff8800402a7f20] do_nmi at ffffffff8152b1db\n#12 [ffff8800402a7f50] nmi at ffffffff8152aaa0\n    [exception RIP: _spin_lock_irqsave+50]\n    RIP: ffffffff8152a202  RSP: ffff8800402a3e88  RFLAGS: 00000097\n    RAX: 00000000000035a1  RBX: ffff8802771d8000  RCX: 00000000000035a0\n    RDX: 0000000000000082  RSI: ffff88027972b680  RDI: ffff8802771d8040\n    RBP: ffff8800402a3e88   R8: ffff88047d316000   R9: ffff88047d317e68\n    R10: 00066ed8f99bdec8  R11: 0000000000000001  R12: ffff8802771d8040\n    R13: 0000000000000000  R14: ffffc90001876000  R15: ffff88027972b680\n    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n#13 [ffff8800402a3e88] _spin_lock_irqsave at ffffffff8152a202\n#14 [ffff8800402a3e90] qla24xx_msix_rsp_q at ffffffffa024e47d [qla2xxx]\n#15 [ffff8800402a3ed0] handle_IRQ_event at ffffffff810e6ec0\n#16 [ffff8800402a3f20] handle_edge_irq at ffffffff810e981e\n#17 [ffff8800402a3f60] handle_irq at ffffffff8100faf9\n#18 [ffff8800402a3f80] do_IRQ at ffffffff81530fdc\n--- <IRQ stack> ---\n#19 [ffff88047d317db8] ret_from_intr at ffffffff8100b9d3\n    [exception RIP: intel_idle+222]\n    RIP: ffffffff812e09be  RSP: ffff88047d317e68  RFLAGS: 00000206\n    RAX: 0000000000000000  RBX: ffff88047d317ed8  RCX: 0000000000000000\n    RDX: 00000000000007bd  RSI: 0000000000000000  RDI: 00000000001e3b6f\n    RBP: ffffffff8100b9ce   R8: 0000000000000002   R9: 00000000000001a4\n    R10: 00066ed8f99bdec8  R11: 0000000000000000  R12: 0000000000000086\n    R13: ffff88047d317dd8  R14: ffff8800402b15e0  R15: 0000000000000000\n    ORIG_RAX: ffffffffffffffbe  CS: 0010  SS: 0018\n#20 [ffff88047d317ee0] cpuidle_idle_call at ffffffff814266f7\n#21 [ffff88047d317f00] cpu_idle at ffffffff81009fc6\ncrash> exit",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140127T07:57:15",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140127T07:57:15",
          "id": "6753537",
          "is_private": "False"
        },
        {
          "count": "26",
          "creator": "ayyanar@netapp.com",
          "text": "vmcore,dmesg and messages file copied here \n\nftp://ftp.netapp.com/pub/home/ayyanar/pub/bz.1033136/crash_bz1033136.tar.bz2",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140127T07:59:07",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140127T07:59:07",
          "id": "6753543",
          "is_private": "False"
        },
        {
          "count": "27",
          "creator": "cdupuis@redhat.com",
          "text": "i looked at the crash in Comment 25 it is a deadlock situation while we're trying to relogin to a port after a timeout (this may be the same issue in Comment 20.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140127T22:06:00",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140127T22:06:00",
          "id": "6757084",
          "is_private": "False"
        },
        {
          "count": "28",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to Chad Dupuis from comment #27)\n> i looked at the crash in Comment 25 it is a deadlock situation while we're\n> trying to relogin to a port after a timeout (this may be the same issue in\n> Comment 20.\n\nWe should open another bugzilla for the symptoms in Comment 27.  We're seeing this on RHEL 7 with Bug 1065880 as well so that fix will apply here as well.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140219T13:49:54",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140219T13:49:54",
          "id": "6835119",
          "is_private": "False"
        },
        {
          "count": "29",
          "creator": "ctatman@redhat.com",
          "text": "*** Bug 1015895 has been marked as a duplicate of this bug. ***",
          "author": "ctatman@redhat.com",
          "creation_time": "20140313T14:07:29",
          "bug_id": "1033136",
          "creator_id": "161143",
          "time": "20140313T14:07:29",
          "id": "6912326",
          "is_private": "False"
        },
        {
          "count": "30",
          "creator": "revers@redhat.com",
          "text": "(In reply to Chad Dupuis from comment #28)\n> (In reply to Chad Dupuis from comment #27)\n> > i looked at the crash in Comment 25 it is a deadlock situation while we're\n> > trying to relogin to a port after a timeout (this may be the same issue in\n> > Comment 20.\n> \n> We should open another bugzilla for the symptoms in Comment 27.  We're\n> seeing this on RHEL 7 with Bug 1065880 as well so that fix will apply here\n> as well.\n\nChad,\n\nDid you want Ayyanar to do this?\n\nRob",
          "author": "revers@redhat.com",
          "creation_time": "20140318T20:20:58",
          "bug_id": "1033136",
          "creator_id": "272069",
          "time": "20140318T20:20:58",
          "id": "6927899",
          "is_private": "False"
        },
        {
          "count": "31",
          "creator": "ayyanar@netapp.com",
          "text": "> Chad,\n> \n> Did you want Ayyanar to do this?\n> \n> Rob\n\nAlready https://bugzilla.redhat.com/show_bug.cgi?id=1070856 opened for this.",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140319T07:42:41",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140319T07:42:41",
          "id": "6929231",
          "is_private": "False"
        },
        {
          "count": "32",
          "creator": "ayyanar@netapp.com",
          "text": "Chad, How do we proceed here? It's been pending for long...",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140326T07:43:16",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140326T07:43:16",
          "id": "6952601",
          "is_private": "False"
        },
        {
          "count": "33",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nNetapp is going to be requesting a zstream for 6.5z as soon as we know if the fix for this bug is going to be in 6.6.  It looks like we now have all the acks for a 6.6 fix. Can I proceed with a 6.5z request?\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20140327T02:37:24",
          "bug_id": "1033136",
          "creator_id": "161143",
          "time": "20140327T02:37:24",
          "id": "6957053",
          "is_private": "True"
        },
        {
          "count": "34",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nCan we proceed with a 6.5z request on this bug?\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20140409T20:39:56",
          "bug_id": "1033136",
          "creator_id": "161143",
          "time": "20140409T20:39:56",
          "id": "6998512",
          "is_private": "False"
        },
        {
          "count": "35",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to Chris Tatman from comment #34)\n> Hi Chad,\n> \n> Can we proceed with a 6.5z request on this bug?\n> \n> Thanks!\n> \n> --Chris\n\nSoon, yes.  There are a couple of fixes from this bug which will be posted upstream shortly.  Once that happens that I can post them for inclusion in 6.6.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140410T11:53:41",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140410T11:53:41",
          "id": "7000408",
          "is_private": "False"
        },
        {
          "count": "36",
          "creator": "ctatman@redhat.com",
          "text": "Hi Chad,\n\nI went ahead and requested the 6.5z for this bug. Hoping we can meet the May 9 deadline for kernel 5.  Please let me know once the fixes are posted upstream and to the rh kernel list so I can start pursuing the acks.\n\nThanks!\n\n--Chris",
          "author": "ctatman@redhat.com",
          "creation_time": "20140424T00:37:51",
          "bug_id": "1033136",
          "creator_id": "161143",
          "time": "20140424T00:37:51",
          "id": "7034352",
          "is_private": "False"
        },
        {
          "count": "37",
          "creator": "dhoward@redhat.com",
          "text": "Hi Chris -\n\nUntil patches are *comitted* in 6.6, we won't be able to pull this into any zstreams.  \n\nI think you need to start setting netapp's expectations for 6.5z kernel 6.",
          "author": "dhoward@redhat.com",
          "creation_time": "20140509T16:52:53",
          "bug_id": "1033136",
          "creator_id": "32221",
          "time": "20140509T16:52:53",
          "id": "7076892",
          "is_private": "True"
        },
        {
          "count": "38",
          "creator": "cdupuis@redhat.com",
          "text": "FYI, RHEL 6.6 qla2xxx has been posted.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140530T14:13:55",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140530T14:13:55",
          "id": "7133738",
          "is_private": "False"
        },
        {
          "count": "39",
          "creator": "ayyanar@netapp.com",
          "text": "(In reply to Chad Dupuis from comment #38)\n> FYI, RHEL 6.6 qla2xxx has been posted.\n\nThanks Chad. Is this driver includes the new firmware for 8G as well right?",
          "author": "ayyanar@netapp.com",
          "creation_time": "20140530T14:24:23",
          "bug_id": "1033136",
          "creator_id": "317542",
          "time": "20140530T14:24:23",
          "id": "7133780",
          "is_private": "False"
        },
        {
          "count": "40",
          "creator": "cdupuis@redhat.com",
          "text": "(In reply to Ayyanar from comment #39)\n> (In reply to Chad Dupuis from comment #38)\n> > FYI, RHEL 6.6 qla2xxx has been posted.\n> \n> Thanks Chad. Is this driver includes the new firmware for 8G as well right?\n\nYes, we are updating the firmware in 6.6 to 7.03.00.",
          "author": "cdupuis@redhat.com",
          "creation_time": "20140530T14:38:24",
          "bug_id": "1033136",
          "creator_id": "295897",
          "time": "20140530T14:38:24",
          "id": "7133850",
          "is_private": "False"
        },
        {
          "count": "41",
          "creator": "coughlan@redhat.com",
          "text": "Ideally we would close this BZ as a duplicate of: \n\n1054299 [QLogic 6.6 Feat] qla2xxx: Update driver to 8.07.00.08.06.6- \n1054301 [QLogic 6.6 Feat] Update 4Gbps firmware to 7.02.00 or greate \n1076497 [QLogic 6.6 Feat] Update 8Gbps firmware to 7.02.00 or greate \n\nbut I will leave it open because of the z-stream request. \n\nSetting it to ON_QA, to match the others.",
          "author": "coughlan@redhat.com",
          "creation_time": "20140612T15:05:43",
          "bug_id": "1033136",
          "creator_id": "84974",
          "time": "20140612T15:05:43",
          "id": "7179049",
          "is_private": "False"
        },
        {
          "count": "42",
          "creator": "dhoward@redhat.com",
          "text": "The 6.5z request here will be addressed via 1110658 (cloned from 1054299).\nFeel free to close this as a dup.",
          "author": "dhoward@redhat.com",
          "creation_time": "20140618T17:18:22",
          "bug_id": "1033136",
          "creator_id": "32221",
          "time": "20140618T17:18:22",
          "id": "7215811",
          "is_private": "True"
        }
      ],
      "cf_show_homepage": "---",
      "platform": "All",
      "version": [
        "6.5"
      ],
      "cc": [
        "ayyanar@netapp.com",
        "bdonahue@redhat.com",
        "bmarson@redhat.com",
        "cdupuis@redhat.com",
        "coughlan@redhat.com",
        "ctatman@redhat.com",
        "czhang@redhat.com",
        "dhoward@redhat.com",
        "fge@redhat.com",
        "giridhar.malavali@qlogic.com",
        "ichute@redhat.com",
        "marting@netapp.com",
        "msnitzer@redhat.com",
        "revers@redhat.com",
        "salmy@redhat.com",
        "syeghiay@redhat.com",
        "xdl-redhat-bugzilla@netapp.com",
        "yanwang@redhat.com"
      ],
      "cf_verified": [
        "Any"
      ],
      "cf_cust_facing": "---",
      "cf_regression_status": "---",
      "cf_environment": "",
      "status": "ON_QA",
      "product": "Red Hat Enterprise Linux 6",
      "cf_verified_branch": "",
      "blocks": [
        "724056",
        "1022765",
        "1056239",
        "786478",
        "846704",
        "882205",
        "891564",
        "947133",
        "1014687"
      ],
      "qa_contact": "xiaoli@redhat.com",
      "tags": [],
      "see_also": [],
      "component": [
        "kernel"
      ],
      "remaining_time": "0.0",
      "sub_components": {},
      "cf_pgm_internal": "",
      "cf_doc_type": "Bug Fix",
      "cf_clone_of": "829739",
      "groups": [
        "devel",
        "netapp",
        "qa",
        "qlogic",
        "redhat",
        "support"
      ],
      "cf_documentation_action": "---",
      "cf_internal_whiteboard": "",
      "target_milestone": "beta",
      "cf_devel_whiteboard": "",
      "is_cc_accessible": "True",
      "cf_type": "Bug",
      "cf_category": "---",
      "url": "",
      "cf_build_id": "",
      "whiteboard": "",
      "cf_crm": "",
      "target_release": [
        "6.6"
      ],
      "alias": [],
      "op_sys": "Linux",
      "flags": [
        {
          "status": "?",
          "name": "qe_test_coverage",
          "modification_date": "20140310T08:27:56",
          "type_id": "318",
          "is_active": "1",
          "creation_date": "20140310T08:27:56",
          "id": "1736183",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "rhel-6.6.0",
          "modification_date": "20140320T14:58:10",
          "type_id": "519",
          "is_active": "1",
          "creation_date": "20131121T15:37:01",
          "id": "1596197",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "blocker",
          "modification_date": "20140320T14:58:10",
          "type_id": "24",
          "is_active": "1",
          "creation_date": "20131121T15:37:01",
          "id": "1596198",
          "setter": "pm-rhel@redhat.com"
        },
        {
          "status": "+",
          "name": "pm_ack",
          "modification_date": "20140305T13:12:41",
          "type_id": "11",
          "is_active": "1",
          "creation_date": "20131121T15:37:01",
          "id": "1596199",
          "setter": "tlavigne@redhat.com"
        },
        {
          "status": "+",
          "name": "devel_ack",
          "modification_date": "20140320T14:52:41",
          "type_id": "10",
          "is_active": "1",
          "creation_date": "20131121T15:37:01",
          "id": "1596200",
          "setter": "revers@redhat.com"
        },
        {
          "status": "+",
          "name": "qa_ack",
          "modification_date": "20140310T08:07:04",
          "type_id": "9",
          "is_active": "1",
          "creation_date": "20131121T15:37:01",
          "id": "1596201",
          "setter": "xiaoli@redhat.com"
        }
      ],
      "last_change_time": "20140618T17:18:22",
      "assigned_to": "cdupuis@redhat.com",
      "update_token": "1403125936-ErPKGt9-9FKRGkkd70jzprgJ9rRvANY2RCaHLeeI_wo",
      "cf_partner": [
        "NetApp",
        "Qlogic"
      ],
      "resolution": "",
      "cf_mount_type": "---",
      "cf_layered_products": []
    }
  ]
}